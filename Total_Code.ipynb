{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["OKomvnszBTcx","uKjnDAWNCBbz","MQ0IFa7yCJKU","KDDsAU1_CM6z","B5eCU0e0CqJH","elNMd4IaC22Z","I68KQqALC7mR","A9sR-fkCCw7x","FDDAIpaQDRW6","jLDMWuNxDVj3"],"authorship_tag":"ABX9TyMnapPOaX4g8a721uFOnKTj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# í¬ë¡¤ë§"],"metadata":{"id":"OKomvnszBTcx"}},{"cell_type":"markdown","source":["### ì•„ë§ˆì¡´ ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘"],"metadata":{"id":"FYulECmuBU-B"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztTkxWqzBIrt"},"outputs":[],"source":["import math\n","import traceback\n","import time\n","import csv\n","import re\n","from selenium import webdriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.chrome.service import Service\n","from selenium.webdriver.chrome.options import Options\n","from selenium.webdriver.common.keys import Keys\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","from webdriver_manager.chrome import ChromeDriverManager\n","import urllib.parse\n","import json\n","\n","# === í¬ë¡¬ ì˜µì…˜ ì„¤ì • ===\n","options = Options()\n","options.add_argument(\"--disable-blink-features=AutomationControlled\")\n","options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n","options.add_experimental_option(\"useAutomationExtension\", False)\n","options.add_argument(\"start-maximized\")\n","options.add_argument(\"disable-infobars\")\n","options.add_argument(\"disable-popup-blocking\")\n","options.add_argument(\n","    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",")\n","\n","service = Service(ChromeDriverManager().install())\n","driver = webdriver.Chrome(service=service, options=options)\n","\n","# === ì‚¬ìš©ìê°€ ì§ì ‘ ì…ë ¥í•  ASIN ë¦¬ìŠ¤íŠ¸ ===\n","# ì—¬ê¸°ì— í¬ë¡¤ë§í•˜ê³  ì‹¶ì€ 30ê°œ (ë˜ëŠ” ì›í•˜ëŠ” ê°œìˆ˜) ì œí’ˆì˜ ASINì„ ì…ë ¥í•˜ì„¸ìš”.\n","ASINS_TO_CRAWL = [\n","    \"B07MJL8NXR\",\n","    \"B08SRSRN7G\", \"B08FY6SCR8\", \"B0CK9QNYZW\", \"B07MFYYZ5B\", \"B0DRPSBQH3\",\n","    \"B07ZP4HBXS\", \"B07W6B8ZLZ\", \"B09VX8WLQQ\", \"B07ZP461TY\", \"B07MCCVZYC\",\n","    \"B0DDYDGCSD\" , \"B0015068TG\", \"B0DY1ZS88N\", \"B0F6F8TBR7\", \"B0DY21L16Y\",\n","    \"B0F3DNXDRZ\", \"B0791ZPVTY\", \"B0F3DRGDGM\", \"B09SGVCNFK\", \"B0791Y2LMB\",\n","    \"B09SGQVPDW\", \"B07NLQ5V6L\", \"B08WHYHPL5\", \"B08WHMBTM6\", \"B0DBRP7NN1\",\n","    \"B09NZ54BR4\", \"B0DBRP2KTM\", \"B0BV9749Q3\", \"B09NZPZ8Y6\", \"B0CPFX647Y\",\n","    \"B0CPFTBWYJ\", \"B005V9UG18\", \"B00AY6S2KU\", \"B003FDC2I2\", \"B0F3PZSNLQ\",\n","    \"B089ZVRR82\", \"B01KITQCW2\", \"B0DN83F6QS\", \"B09V4P2DVF\", \"B01KITQG0A\",\n","    \"B01KITQEPM\", \"B07ZZCM9SZ\", \"B0D6DKR58D\", \"B09G71BM3N\", \"B0854W9ND4\",\n","    \"B07ZZCDZST\", \"B0D7TFG59T\", \"B0B69YRPXH\", \"B08B2XCK56\", \"B0B75R8Z7X\",\n","    \"B01HOPJ2WU\", \"B07BL5ZHF9\", \"B07BL5BT2V\", \"B07BL5G7TC\", \"B0B6WRW3GX\",\n","    \"B01HOPJ8V0\", \"B01HOPJ3OC\", \"B0DS6MT455\", \"B0DS6LWMKJ\", \"B0DS6LGSV6\",\n","    \"B0D8MBS6XX\", \"B0D8M7RSHX\", \"B0D8M7K536\", \"B0DTMHYMG8\", \"B09V1S9168\",\n","    \"B0D823BK53\", \"B09V1S3V12\", \"B0DS6KMXHH\", \"B0DS6KYV4T\", \"B0DS6LSXXP\",\n","    \"B074P98SV7\", \"B07NH274NS\", \"B0C6YLVTHP\", \"B074P9D8JK\", \"B0DLCNXJD1\",\n","    \"B0DLBVHTB5\", \"B0DLCDBY96\", \"B0DHWD5WBM\", \"B0DHWDLW5X\", \"B0036F576I\",\n","    \"B0DQYV1Y51\", \"B06XWZHPN8\", \"B00166BBXW\", \"B0BSXXCD97\", \"B0034L4YGI\",\n","    \"B084WB9T61\", \"B0CFGBC9G2\", \"B01769TWGA\", \"B0036FBZPK\", \"B0CCWXBD5D\",\n","    \"B07BDS1YFX\", \"B085LSZCJ7\", \"B085LVFFKM\", \"B08B2J4DKZ\", \"B08TJ4LQ1N\",\n","    \"B08B7B3CF8\", \"B09LZZL1FF\", \"B0CTJ7G7N3\", \"B08GLMGVJQ\", \"B09B7Z7WGG\",\n","    \"B09PFK7H3W\", \"B09BC7V1JJ\", \"B08GLMT52Z\", \"B0BCSZPGQ3\", \"B079WNBTSH\",\n","    \"B0CM43QLM7\", \"B0B5M6RMFD\", \"B09BBWYD8X\", \"B0015068PA\", \"B0C5YVDRVM\",\n","    \"B09BBY5GPD\", \"B00BKL4KKY\", \"B0CDHNX333\", \"B00HKF9KFE\", \"B00BYOW7VG\",\n","    \"B0BKH8LN24\", \"B0BKH6MYD1\", \"B0CP8HZW65\", \"B0BKH4JXTM\", \"B01DDIRDZA\",\n","    \"B09GJVN2YZ\", \"B0DZY51RWP\", \"B0DZ8KV821\", \"B08QQKRWPR\", \"B08QQZ71P6\",\n","    \"B0BNFB7PSG\", \"B09NCJYC6K\", \"B09NCGMWX6\", \"B08YB4JQN4\", \"B08QQBGL95\",\n","    \"B08YS3TWTT\"\n","]\n","\n","# ëª¨ë“  ì œí’ˆì˜ ê¸°ë³¸ ë¦¬ë·° URLì— formatType=current_format ê³ ì •\n","# ë¦¬ë·° íŒŒì¼ëª…ì— ì‚¬ìš©ë  Flavor ì‹ë³„ì.\n","FLAVOR_IDENTIFIER_FOR_FILENAME = \"current_format_reviews\"\n","\n","\n","def crawl_product_info_and_reviews_combined(asin, review_star_filters=None):\n","    if review_star_filters is None:\n","        review_star_filters = [\"5\", \"4\", \"3\", \"2\", \"1\"]  # ë³„ì  í•„í„° ìˆœì„œ\n","\n","    star_filter_map = {\n","        \"5\": \"five_star\",\n","        \"4\": \"four_star\",\n","        \"3\": \"three_star\",\n","        \"2\": \"two_star\",\n","        \"1\": \"one_star\",\n","    }\n","\n","    product_data = {}\n","    product_url = f\"https://www.amazon.com/dp/{asin}\"\n","\n","    print(f\"\\n--- ASIN: {asin} ì œí’ˆ ì •ë³´ ë° ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘ ---\")\n","\n","    driver.get(product_url)\n","    time.sleep(3)\n","\n","    try:\n","        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"productTitle\")))\n","    except Exception as e:\n","        print(f\"ì˜¤ë¥˜: ì œí’ˆ {asin} ìƒì„¸í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” productTitle ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}\")\n","        return None\n","\n","    if \"/pharmacy/\" in driver.current_url:\n","        print(f\"DEBUG: ì œí’ˆ {asin} pharmacy ë¦¬ë””ë ‰ì…˜ ê°ì§€, ì´ ì œí’ˆì€ ìˆ˜ì§‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n","        return None\n","\n","    # ì œí’ˆëª…\n","    try:\n","        product_title = driver.find_element(By.ID, \"productTitle\").text.strip()\n","    except:\n","        product_title = \"N/A\"\n","        print(f\"DEBUG: ASIN {asin} ì œí’ˆëª… ì¶”ì¶œ ì‹¤íŒ¨\")\n","\n","    # ëŒ€í‘œ ì´ë¯¸ì§€ URL\n","    try:\n","        img_url = driver.find_element(By.ID, \"landingImage\").get_attribute(\"src\")\n","    except:\n","        img_url = \"N/A\"\n","        print(f\"DEBUG: ASIN {asin} ì´ë¯¸ì§€ URL ì¶”ì¶œ ì‹¤íŒ¨\")\n","\n","    # ê°€ê²©\n","    try:\n","        price_whole = driver.find_element(By.CSS_SELECTOR, \"span.a-price-whole\").text.strip()\n","        price_fraction = driver.find_element(By.CSS_SELECTOR, \"span.a-price-fraction\").text.strip()\n","        price_whole_clean = price_whole.replace(\",\", \"\")\n","        price = f\"${price_whole_clean}.{price_fraction}\"\n","        try:\n","            price_per_count = driver.find_element(By.CSS_SELECTOR, \"span.a-price a-price-symbol + span\").text.strip()\n","            price += f\" ({price_per_count})\"\n","        except:\n","            pass\n","    except:\n","        price = \"N/A\"\n","        print(f\"DEBUG: ASIN {asin} ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨\")\n","\n","    # Product Overview\n","    product_overview = {}\n","    try:\n","        overview_table = driver.find_element(By.ID, \"productOverview_feature_div\")\n","        rows = overview_table.find_elements(By.CSS_SELECTOR, \"tr\")\n","        for row in rows:\n","            try:\n","                th = row.find_element(By.TAG_NAME, \"th\").text.strip()\n","                td = row.find_element(By.TAG_NAME, \"td\").text.strip()\n","                product_overview[th] = td\n","            except:\n","                continue\n","    except:\n","        print(f\"DEBUG: ASIN {asin} Product Overview ì¶”ì¶œ ì‹¤íŒ¨\")\n","\n","    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","    time.sleep(4)\n","\n","    # Product Description\n","    product_description = \"\"\n","    try:\n","        desc_div = driver.find_element(By.ID, \"productDescription\")\n","        product_description = desc_div.text.strip()\n","    except:\n","        pass\n","    if not product_description:\n","        try:\n","            alt_desc = driver.find_element(By.CSS_SELECTOR, \"div#feature-bullets ~ div.a-section.a-spacing-small.a-spacing-top-small\")\n","            product_description = alt_desc.text.strip()\n","        except:\n","            print(f\"DEBUG: ASIN {asin} Product Description ì¶”ì¶œ ì‹¤íŒ¨\")\n","            pass\n","\n","    # About this item\n","    about_items = []\n","    try:\n","        about_div = driver.find_element(By.ID, \"feature-bullets\")\n","        bullet_points = about_div.find_elements(By.CSS_SELECTOR, \"span.a-list-item\")\n","        for bp in bullet_points:\n","            txt = bp.text.strip()\n","            if txt:\n","                about_items.append(txt)\n","    except:\n","        try:\n","            about_div = driver.find_element(By.ID, \"feature-bullets_feature_div\")\n","            bullet_points = about_div.find_elements(By.CSS_SELECTOR, \"ul li\")\n","            for bp in bullet_points:\n","                txt = bp.text.strip()\n","                if txt:\n","                    about_items.append(txt)\n","        except:\n","            print(f\"DEBUG: ASIN {asin} About this item ì¶”ì¶œ ì‹¤íŒ¨\")\n","            pass\n","\n","    # Important Information\n","    important_info = {}\n","    try:\n","        important_div = None\n","        try:\n","            important_div = driver.find_element(By.ID, \"important-information\")\n","        except:\n","            important_div = driver.find_element(By.ID, \"importantInformation_feature_div\")\n","\n","        if important_div:\n","            sections = important_div.find_elements(By.CSS_SELECTOR, \"div.a-section\")\n","            for section in sections:\n","                try:\n","                    heading = section.find_element(By.TAG_NAME, \"h3\").text.strip()\n","                    content = section.find_element(By.TAG_NAME, \"p\").text.strip()\n","                    important_info[heading] = content\n","                except:\n","                    text = section.text.strip()\n","                    if text:\n","                        important_info[f\"section_{len(important_info)+1}\"] = text\n","    except:\n","        print(f\"DEBUG: ASIN {asin} Important Information ì¶”ì¶œ ì‹¤íŒ¨\")\n","        pass\n","\n","    # ì œí’ˆ ì •ë³´ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±\n","    product_data = {\n","        \"asin\": asin,\n","        \"title\": product_title,\n","        \"image_url\": img_url,\n","        \"price\": price,\n","        \"product_overview\": json.dumps(product_overview, ensure_ascii=False), # JSON ë¬¸ìì—´ë¡œ ì €ì¥\n","        \"product_description\": product_description,\n","        \"about_this_item\": json.dumps(about_items, ensure_ascii=False),     # JSON ë¬¸ìì—´ë¡œ ì €ì¥\n","        \"important_information\": json.dumps(important_info, ensure_ascii=False), # JSON ë¬¸ìì—´ë¡œ ì €ì¥\n","        \"product_url\": product_url,\n","    }\n","\n","    print(f\"âœ… ASIN {asin} ì œí’ˆ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ.\")\n","\n","    # ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘\n","    all_reviews_raw = [] # ëª¨ë“  ë³„ì ì˜ ë¦¬ë·° ë°ì´í„° ( Flavor, Reviewer, Rating, Title, Body )\n","\n","    # ëª¨ë“  ì œí’ˆì˜ ê¸°ë³¸ ë¦¬ë·° URLì— formatType=current_format ê³ ì •\n","    base_review_url = f\"https://www.amazon.com/product-reviews/{asin}/ref=cm_cr_arp_d_viewopt_fmt?ie=UTF8&reviewerType=all_reviews&formatType=current_format\"\n","\n","\n","    for star_filter in review_star_filters:\n","        reviews_this_star = []\n","\n","        filter_param = star_filter_map.get(star_filter, None)\n","        if not filter_param:\n","            continue\n","\n","        review_url_with_stars = f\"{base_review_url}&filterByStar={filter_param}\"\n","\n","        driver.get(review_url_with_stars)\n","        time.sleep(3)\n","\n","        try:\n","            WebDriverWait(driver, 10).until(\n","                EC.presence_of_element_located((By.CSS_SELECTOR, \"span[data-hook='review-body']\"))\n","            )\n","        except Exception as e:\n","            print(f\"âŒ {star_filter}â­ ë¦¬ë·° ì´ˆê¸° ë¡œë“œ ì‹¤íŒ¨ for ASIN {asin} (ì˜¤ë¥˜: {e})\")\n","            continue\n","\n","        while True:\n","            try:\n","                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","                time.sleep(3)\n","\n","                review_blocks = driver.find_elements(By.CSS_SELECTOR, \"li[data-hook='review']\")\n","\n","                for review_block in review_blocks:\n","                    try:\n","                        rating_text = review_block.find_element(By.CSS_SELECTOR, \"i[data-hook='review-star-rating'] span.a-icon-alt\").text.strip()\n","                    except:\n","                        rating_text = \"N/A\"\n","\n","                    title = \"N/A\"\n","                    translated_title_text = \"\"\n","                    original_title_text = \"\"\n","\n","                    try:\n","                        translated_title_elem = review_block.find_element(By.CSS_SELECTOR, \"a[data-hook='review-title'] .cr-translated-review-content\")\n","                        translated_title_text = translated_title_elem.get_attribute('textContent').strip() if translated_title_elem else \"\"\n","                    except:\n","                        pass\n","\n","                    try:\n","                        original_title_elem = review_block.find_element(By.CSS_SELECTOR, \"a[data-hook='review-title']\")\n","                        original_title_text = original_title_elem.text.strip()\n","                    except:\n","                        pass\n","\n","                    if translated_title_text:\n","                        title = translated_title_text\n","                    elif original_title_text:\n","                        title = original_title_text\n","\n","\n","                    try:\n","                        reviewer = review_block.find_element(By.CSS_SELECTOR, \"span.a-profile-name\").text.strip()\n","                    except:\n","                        reviewer = \"N/A\"\n","\n","                    body = \"N/A\"\n","                    translated_body_text = \"\"\n","                    original_body_text = \"\"\n","                    try:\n","                        translated_body_elem = review_block.find_element(By.CSS_SELECTOR, \"span.cr-translated-review-content\")\n","                        translated_body_text = translated_body_elem.get_attribute('textContent').strip() if translated_body_elem else \"\"\n","                    except:\n","                        pass\n","\n","                    try:\n","                        original_body_elem = review_block.find_element(By.CSS_SELECTOR, \"span[data-hook='review-body']\")\n","                        original_body_text = original_body_elem.text.strip()\n","                    except:\n","                        pass\n","\n","                    if translated_body_text:\n","                        body = translated_body_text\n","                    elif original_body_text:\n","                        body = original_body_text\n","\n","\n","                    try:\n","                        format_strip = review_block.find_element(By.CSS_SELECTOR, \"a[data-hook='format-strip']\").text\n","                        flavor = \"N/A\"\n","                        for segment in format_strip.split(\"|\"):\n","                            if \"Flavor Name\" in segment:\n","                                flavor = segment.strip().replace(\"Flavor Name:\", \"\").strip()\n","                                break\n","                    except:\n","                        flavor = \"N/A\"\n","\n","                    review_data_row = {\n","                        \"Flavor\": flavor,\n","                        \"Reviewer\": reviewer,\n","                        \"Rating\": rating_text,\n","                        \"Title\": title,\n","                        \"Body\": body\n","                    }\n","                    if review_data_row not in reviews_this_star:\n","                        reviews_this_star.append(review_data_row)\n","\n","                try:\n","                    next_btn = driver.find_element(By.CSS_SELECTOR, \"li.a-last a\")\n","                    next_btn.click()\n","                    time.sleep(3)\n","                except:\n","                    break\n","\n","            except Exception as e:\n","                print(f\"ì˜¤ë¥˜: ë¦¬ë·° í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ASIN: {asin}, ë³„ì : {star_filter}â­): {e}\")\n","                traceback.print_exc()\n","                break\n","\n","        print(f\"âœ… ASIN {asin}ì˜ 'Current Format'ì— ëŒ€í•œ {star_filter}â­ ë¦¬ë·° {len(reviews_this_star)}ê°œ ìˆ˜ì§‘ë¨\")\n","        all_reviews_raw += reviews_this_star\n","\n","    print(f\"âœ… ASIN {asin}ì˜ ì´ {len(all_reviews_raw)}ê±´ì˜ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ.\")\n","\n","    # --- ì œí’ˆ ì •ë³´ì™€ ë¦¬ë·°ë¥¼ í•˜ë‚˜ì˜ CSV íŒŒì¼ë¡œ ì €ì¥ ---\n","    combined_filename = f\"amazon_product_and_reviews_combined_{asin}_{FLAVOR_IDENTIFIER_FOR_FILENAME}.csv\"\n","\n","    # CSV í—¤ë” ì •ì˜\n","    # ì œí’ˆ ì •ë³´ í•„ë“œ (ë¦¬ë·°ì™€ ê²¹ì¹˜ì§€ ì•Šê²Œ 'Product_' ì ‘ë‘ì‚¬ ì‚¬ìš©)\n","    product_fields = [f\"Product_{k}\" for k in product_data.keys() if k not in [\"asin\"]]\n","    # ë¦¬ë·° í•„ë“œ (ê¸°ì¡´ ì´ë¦„ ì‚¬ìš©)\n","    review_fields = [\"Flavor\", \"Reviewer\", \"Rating\", \"Title\", \"Body\"]\n","\n","    # ASINì€ ëª¨ë“  í–‰ì— ê³µí†µìœ¼ë¡œ ë“¤ì–´ê°ˆ ê²ƒì´ë¯€ë¡œ ë”°ë¡œ ì •ì˜\n","    fieldnames = [\"ASIN\"] + product_fields + review_fields\n","\n","    with open(combined_filename, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n","        writer = csv.DictWriter(f, fieldnames=fieldnames)\n","        writer.writeheader()\n","\n","        # ì²« ë²ˆì§¸ í–‰ì— ì œí’ˆ ì •ë³´ì™€ ì²« ë²ˆì§¸ ë¦¬ë·°ë¥¼ í•¨ê»˜ ì €ì¥ (ë¦¬ë·°ê°€ ìˆë‹¤ë©´)\n","        # ë¦¬ë·°ê°€ ì—†ìœ¼ë©´ ì œí’ˆ ì •ë³´ë§Œ ì €ì¥\n","        first_row_data = {\"ASIN\": asin}\n","\n","        # ì œí’ˆ ì •ë³´ ì¶”ê°€\n","        for k, v in product_data.items():\n","            if k != \"asin\": # ASINì€ ì´ë¯¸ ìœ„ì—ì„œ ì¶”ê°€í–ˆìœ¼ë¯€ë¡œ ì œì™¸\n","                first_row_data[f\"Product_{k}\"] = v\n","\n","        if all_reviews_raw:\n","            # ì²« ë²ˆì§¸ ë¦¬ë·° ë°ì´í„° ì¶”ê°€\n","            first_review = all_reviews_raw[0]\n","            for k, v in first_review.items():\n","                first_row_data[k] = v\n","            writer.writerow(first_row_data)\n","\n","            # ë‚˜ë¨¸ì§€ ë¦¬ë·° ë°ì´í„° ì¶”ê°€ (ì œí’ˆ ì •ë³´ëŠ” ë¹„ì›Œë‘ê³  ë¦¬ë·° ë°ì´í„°ë§Œ ì±„ì›€)\n","            for review_row in all_reviews_raw[1:]:\n","                row_to_write = {\"ASIN\": asin} # ASINì€ ê³„ì† í¬í•¨\n","                for k, v in review_row.items():\n","                    row_to_write[k] = v\n","                writer.writerow(row_to_write)\n","        else:\n","            # ë¦¬ë·°ê°€ ì—†ëŠ” ê²½ìš° ì œí’ˆ ì •ë³´ë§Œ ì²« í–‰ì— ì €ì¥\n","            writer.writerow(first_row_data)\n","\n","    print(f\"âœ… ì œí’ˆ {asin} ì •ë³´ ë° ë¦¬ë·° ì´ {len(all_reviews_raw) + (1 if product_data else 0)}ê±´ í†µí•© ì €ì¥ ì™„ë£Œ: {combined_filename}\")\n","\n","    return product_data # ì´ í•¨ìˆ˜ëŠ” ë” ì´ìƒ ì œí’ˆ ì •ë³´ë§Œ ë°˜í™˜í•˜ì§€ ì•Šì§€ë§Œ, í˜¸ì¶œ êµ¬ì¡° ìœ ì§€ë¥¼ ìœ„í•´ product_data ë°˜í™˜.\n","\n","# === ë©”ì¸ ì‹¤í–‰ ë¡œì§ ===\n","driver.get(\"https://www.amazon.com/\")\n","input(\"Amazon ì›¹í˜ì´ì§€ì—ì„œ ë¡œê·¸ì¸ ì™„ë£Œ í›„, ì½˜ì†”ì— Enter í‚¤ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...\")\n","\n","print(f\"\\n--- {len(ASINS_TO_CRAWL)}ê°œì˜ ASINì— ëŒ€í•œ ì œí’ˆ ì •ë³´ ë° ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘ ---\")\n","\n","MAX_RETRIES = 3\n","\n","for i, asin in enumerate(ASINS_TO_CRAWL):\n","    print(f\"\\n[{i+1}/{len(ASINS_TO_CRAWL)}] ASIN: {asin} í¬ë¡¤ë§ ì¤‘...\")\n","\n","    retries = 0\n","    while retries < MAX_RETRIES:\n","        try:\n","            # í•¨ìˆ˜ ì´ë¦„ ë³€ê²½: crawl_product_info_and_reviews_combined\n","            result = crawl_product_info_and_reviews_combined(asin)\n","            if result:\n","                print(f\"âœ… ASIN {asin} í¬ë¡¤ë§ ì„±ê³µ.\")\n","                break\n","            else:\n","                print(f\"âš ï¸ ASIN {asin} ì œí’ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¬ì‹œë„í•©ë‹ˆë‹¤. (ì‹œë„ {retries+1}/{MAX_RETRIES})\")\n","                retries += 1\n","                time.sleep(5)\n","        except Exception as e:\n","            print(f\"âŒ ASIN {asin} í¬ë¡¤ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}\")\n","            traceback.print_exc()\n","            retries += 1\n","            if retries < MAX_RETRIES:\n","                print(f\"ì¬ì‹œë„ ì¤‘... (ì‹œë„ {retries}/{MAX_RETRIES})\")\n","                time.sleep(10)\n","            else:\n","                print(f\"âŒ ASIN {asin} í¬ë¡¤ë§ {MAX_RETRIES}íšŒ ì¬ì‹œë„ ì‹¤íŒ¨. ì´ ASINì€ ê±´ë„ˆí‚µë‹ˆë‹¤.\")\n","\n","    if retries == MAX_RETRIES and not result:\n","        print(f\"âš ï¸ ASIN {asin} í¬ë¡¤ë§ ìµœì¢… ì‹¤íŒ¨. ê±´ë„ˆí‚µë‹ˆë‹¤.\")\n","\n","print(\"\\n--- ëª¨ë“  ì œí’ˆ ì •ë³´ ë° ë¦¬ë·° í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ---\")\n","\n","driver.quit()"]},{"cell_type":"markdown","source":["### ë„¤ì´ë²„ ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘"],"metadata":{"id":"GTdTNPmxBm4e"}},{"cell_type":"code","source":["options = webdriver.ChromeOptions()  # í¬ë¡¬ ì˜µì…˜ ê°ì²´ ìƒì„±\n","options.add_argument(\"window-size-1920x1080\")  # ì „ì²´í™”ë©´\n","options.add_argument(\"disable-gpu\")\n","options.add_argument(\"disable-infobars\")\n","options.add_argument(\"--disable-extensions\")\n","options.add_argument(\"--no-sandbox\")"],"metadata":{"id":"UW_G4MTgBo4r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chrome_driver_path = \"driver_path\"\n","service = Service(executable_path = chrome_driver_path)"],"metadata":{"id":"RJS-7XqBBwZg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["driver = webdriver.Chrome(service = service, options = options)\n","driver.implicitly_wait(3)"],"metadata":{"id":"xaiF1zCoBxwO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["driver.get(\"URL_PATH\")\n","time.sleep(3)"],"metadata":{"id":"lZ13ZJCJByo8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","time.sleep(5)"],"metadata":{"id":"aPusV7-kBzwx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["driver.find_element(By.CSS_SELECTOR, \"#content > div > div._3CTsMZymJs > div:nth-child(3) > div._27jmWaPaKy > ul > li:nth-child(2) > a\").click()\n","time.sleep(5)"],"metadata":{"id":"N-OZ0OZ_B0tl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["driver.find_element(By.CSS_SELECTOR, \"#REVIEW > div > div._2LvIMaBiIO > div._3aC7jlfVdk > div._1txuie7UTH > ul > li:nth-child(2) > a\").click()\n","time.sleep(3)"],"metadata":{"id":"ZVGs96-7B1nJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# í˜„ì¬ í˜ì´ì§€\n","page_num = 1\n","page_ctl = 3\n","\n","write_dt_lst = []\n","item_nm_lst = []\n","content_lst = []\n","\n","# ë‚ ì§œ\n","date_cut = (datetime.now() - timedelta(days=365)).strftime('%Y%m%d')\n","\n","while True:\n","    if page_num == 26:\n","        print(\"500ê°œ ìˆ˜ì§‘ ì™„ë£Œ\")\n","        break\n","\n","    print(f'start : {page_num} page ìˆ˜ì§‘ ì¤‘, page_ctl:{page_ctl}')\n","\n","    # 1. ì…€ë ˆë‹ˆì›€ìœ¼ë¡œ html ê°€ì ¸ì˜¤ê¸°\n","    html_source = driver.page_source\n","\n","    # 2. bs4ë¡œ html íŒŒì‹±\n","    soup = BeautifulSoup(html_source, 'html.parser')\n","    time.sleep(0.5)\n","\n","    # 3. ë¦¬ë·° ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n","    reviews = soup.findAll('li', {'class': 'BnwL_cs1av'})\n","\n","    # 4. í•œ í˜ì´ì§€ ë‚´ì—ì„œ ìˆ˜ì§‘ ê°€ëŠ¥í•œ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n","    for review in range(len(reviews)):\n","        try:\n","            # 4-1. ë¦¬ë·° ì‘ì„±ì¼ì ìˆ˜ì§‘\n","            write_dt_raw = reviews[review].findAll('span', {'class': '_2L3vDiadT9'})[0].get_text()\n","            write_dt = datetime.strptime(write_dt_raw, '%y.%m.%d.').strftime('%Y%m%d')\n","        except Exception:\n","            write_dt = ''\n","\n","        # 4-2. ìƒí’ˆëª… ìˆ˜ì§‘\n","        try:\n","            item_nm_divs = reviews[review].findAll('div', {'class': '_2FXNMst_ak'})\n","            if item_nm_divs:\n","                item_nm_info_raw = item_nm_divs[0].get_text()\n","\n","                # dl íƒœê·¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì—†ì„ ìˆ˜ë„ ìˆìŒ)\n","                dl_tag = item_nm_divs[0].find('dl', {'class': 'XbGQRlzveO'})\n","                item_nm_info_for_del = dl_tag.get_text() if dl_tag else ''\n","\n","                # í…ìŠ¤íŠ¸ ì •ì œ\n","                item_nm_info = re.sub(item_nm_info_for_del, '', item_nm_info_raw)\n","\n","                # 'ì œí’ˆ ì„ íƒ: ' ìœ„ì¹˜ ì°¾ê¸°\n","                str_start_idx = item_nm_info.find('ì œí’ˆ ì„ íƒ: ')\n","                if str_start_idx != -1:\n","                    item_nm = item_nm_info[str_start_idx + len('ì œí’ˆ ì„ íƒ: '):].strip()\n","                else:\n","                    item_nm = item_nm_info.strip()\n","            else:\n","                item_nm = ''\n","        except Exception:\n","            item_nm = ''\n","\n","        # 4-3. ë¦¬ë·°ë‚´ìš© ìˆ˜ì§‘\n","        try:\n","            review_div = reviews[review].findAll('div', {'class': '_1kMfD5ErZ6'})\n","            if review_div:\n","                span_tag = review_div[0].find('span', {'class': '_2L3vDiadT9'})\n","                if span_tag:\n","                    review_content_raw = span_tag.get_text()\n","                    review_content = re.sub(' +', ' ', re.sub('\\n', ' ', review_content_raw))\n","                else:\n","                    review_content = ''\n","            else:\n","                review_content = ''\n","        except Exception:\n","            review_content = ''\n","\n","        # 4-4. ìˆ˜ì§‘ë°ì´í„° ì €ì¥\n","        write_dt_lst.append(write_dt)\n","        item_nm_lst.append(item_nm)\n","        content_lst.append(review_content)\n","\n","    # 5. ë¦¬ë·° ìˆ˜ì§‘ì¼ì ê¸°ì¤€ ë°ì´í„° í™•ì¸ (ìµœê·¼ 1ë…„ì¹˜ë§Œ ìˆ˜ì§‘)\n","    if write_dt_lst and write_dt_lst[-1] < date_cut:\n","        break\n","\n","    # 6. í˜ì´ì§€ ì´ë™\n","    try:\n","        driver.find_element(By.CSS_SELECTOR, f'#REVIEW > div > div._2LvIMaBiIO > div._2g7PKvqCKe > div > div > a:nth-child({page_ctl})').click()\n","        time.sleep(5)\n","    except Exception as e:\n","        print(f\"í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}\")\n","        break\n","\n","    page_num += 1\n","    page_ctl += 1\n","    if page_num % 10 == 1:\n","        page_ctl = 3\n","\n","print('done')"],"metadata":{"id":"aJovwPOLB2kx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result_df = pd.DataFrame({\n","              'Item_info' : item_nm_lst,\n","              'Content' : content_lst,\n","              'Date' : write_dt_lst })"],"metadata":{"id":"Fe7ks84AB4Wd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ì˜¥ì…˜ ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘"],"metadata":{"id":"oIub2dOGB5qH"}},{"cell_type":"code","source":["from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# ğŸ”½ ë³µì‚¬í•œ <ul class=\"list-item\"> ì „ì²´ HTMLì„ ì—¬ê¸°ì— ë¶™ì—¬ë„£ê¸°\n","html = \"\"\"\n","\n","\"\"\"\n","\n","# ğŸŒ¿ íŒŒì‹± ì‹œì‘\n","soup = BeautifulSoup(html, \"html.parser\")\n","review_items = soup.select(\"li.list-item\")\n","\n","data = []\n","\n","for item in review_items:\n","    review_text = item.select_one('.box__review-text .text')\n","    writer = item.select_one('.text__writer')\n","    date = item.select_one('.text__date')\n","    rating = item.select_one('.sprite__vip.image__star .for-a11y')\n","    rating_value = ''\n","\n","    if rating:\n","        text = rating.get_text(strip=True)\n","        # ì˜ˆ: 'ì´ìš©ì í‰ì  4ì ' â†’ ìˆ«ìë§Œ ì¶”ì¶œ\n","        import re\n","        match = re.search(r'(\\d(?:\\.\\d)?)ì ', text)\n","        if match:\n","            rating_value = match.group(1)\n","\n","    data.append({\n","        'ì œí’ˆëª…': 'ì œí’ˆëª…',\n","        'ìƒí’ˆë²ˆí˜¸':'',\n","        'ë¦¬ë·°': review_text.get_text(strip=True) if review_text else '',\n","        'ì‘ì„±ì': writer.get_text(strip=True) if writer else '',\n","        'ë‚ ì§œ': date.get_text(strip=True) if date else '',\n","        'ë³„ì ': rating_value\n","    })\n","\n","\n","# ğŸ§¾ CSVë¡œ ì €ì¥\n","df = pd.DataFrame(data)\n","df.to_csv(\"ì œí’ˆëª….csv\", index=False, encoding=\"utf-8-sig\")\n","print(f\"âœ… ì €ì¥ ì™„ë£Œ: 11st_reviews_ul.csv (ì´ {len(data)}ê°œì˜ ë¦¬ë·°ê°€ ëª¨ì˜€ìŠµë‹ˆë‹¤.)\")\n"],"metadata":{"id":"2aYrE4I7B7DR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ë°ì´í„° ë¶„ì„"],"metadata":{"id":"uKjnDAWNCBbz"}},{"cell_type":"markdown","source":["### ì›Œë“œ í´ë¼ìš°ë“œ"],"metadata":{"id":"MQ0IFa7yCJKU"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","from mecab import MeCab # 'mecab-python3' ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì§ì ‘ MeCabì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n","import re\n","import os\n","from collections import Counter # ë‹¨ì–´ ë¹ˆë„ìˆ˜ë¥¼ ì„¸ê¸° ìœ„í•´ Counterë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.\n","\n","# matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •\n","# í°íŠ¸ ê²½ë¡œê°€ ì •í™•í•œì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.\n","plt.rc('font', family='NanumBarunGothic')\n","plt.rcParams['axes.unicode_minus'] = False\n","\n","# Mecab ì´ˆê¸°í™” (ì‚¬ì „ ê²½ë¡œë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ì¤ë‹ˆë‹¤.)\n","try:\n","    mecab = MeCab()\n","    print(\"MeCabì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","except Exception as e:\n","    print(f\"MeCab ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n","    # MeCab ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\n","    exit()\n","\n","# --- 2. í•¨ìˆ˜ ì •ì˜ ---\n","# í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë¥¼ ì¶”ì¶œí•˜ê³  ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜\n","def get_nouns(text, stop_words):\n","    text = str(text) # ì…ë ¥ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì—ëŸ¬ ë°©ì§€\n","    # í•œê¸€, ì˜ì–´, ìˆ«ìë§Œ ë‚¨ê¸°ê³  íŠ¹ìˆ˜ë¬¸ì ì œê±°\n","    text = re.sub('[^ê°€-í£a-zA-Z0-9\\s]', '', text)\n","\n","    # ëª…ì‚¬ë§Œ ì¶”ì¶œ\n","    nouns = mecab.nouns(text)\n","\n","    # í•œ ê¸€ì ëª…ì‚¬ ë° ë¶ˆìš©ì–´ ì œê±°\n","    # `stop_words` ë¦¬ìŠ¤íŠ¸ë¥¼ ì™¸ë¶€ì—ì„œ ë°›ì•„ì„œ ë” ìœ ì—°í•˜ê²Œ ê´€ë¦¬í•©ë‹ˆë‹¤.\n","    filtered_nouns = [n for n in nouns if len(n) > 1 and n not in stop_words]\n","\n","    return filtered_nouns\n","\n","# --- 3. ë©”ì¸ ë¡œì§ ì‹¤í–‰ ---\n","def main():\n","    # CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½)\n","    file_path = '/content/drive/MyDrive/ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”/1. í‚¤ì›Œë“œ ì¶”ì¶œ/protein_1.csv'\n","\n","    try:\n","        df = pd.read_csv(file_path)\n","        print(\"CSV íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","    except FileNotFoundError:\n","        print(f\"ì˜¤ë¥˜: {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n","        return # íŒŒì¼ì´ ì—†ìœ¼ë©´ í•¨ìˆ˜ ì¢…ë£Œ\n","\n","    # ë„¤ê°€ ì¶”ê°€í–ˆë˜ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë”°ë¡œ ì •ì˜\n","    stop_words = ['ì •ë„', 'ë•Œë¬¸', 'ë¶€ë¶„', 'ìƒê°', 'ì œí’ˆ', 'ê²ƒ', 'ì €', 'ë‹¤', 'ë°°ì†¡']\n","\n","    # 'ë¦¬ë·°' ì»¬ëŸ¼ì˜ ëª¨ë“  ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹©ë‹ˆë‹¤.\n","    # ê²°ì¸¡ì¹˜(NaN)ëŠ” ì œì™¸í•˜ê³  í•©ì³ì¤ë‹ˆë‹¤.\n","    all_reviews = ' '.join(df['ë¦¬ë·°'].dropna().astype(str))\n","\n","    # í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ë¹„ì–´ ìˆëŠ”ì§€ í™•ì¸\n","    if not all_reviews.strip():\n","        print(\"ì˜¤ë¥˜: 'ë¦¬ë·°' ì»¬ëŸ¼ì— ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ì–´ ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","        return\n","\n","    # ëª…ì‚¬ ì¶”ì¶œ\n","    nouns = get_nouns(all_reviews, stop_words)\n","\n","    # ì¶”ì¶œëœ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆëŠ”ì§€ í™•ì¸ (í° ë°”íƒ• ë¬¸ì œ ë°©ì§€)\n","    if not nouns:\n","        print(\"ì˜¤ë¥˜: ëª…ì‚¬ ì¶”ì¶œ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","        return\n","\n","    # ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ì˜ ë¹ˆë„ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜\n","    word_counts = Counter(nouns)\n","\n","    # ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±\n","    wordcloud = WordCloud(\n","        font_path='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\n","        width=800,\n","        height=400,\n","        max_words=50,\n","        background_color='white'\n","    )\n","    # ë¹ˆë„ìˆ˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±\n","    wordcloud.generate_from_frequencies(word_counts)\n","\n","    # ì‹œê°í™”\n","    plt.figure(figsize=(10, 8))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis('off')\n","    plt.show()\n","\n","    # ì´ë¯¸ì§€ ì €ì¥\n","    # ì €ì¥í•  ê²½ë¡œì™€ íŒŒì¼ëª…ì„ ì„¤ì •í•´ ì£¼ì„¸ìš”.\n","    save_path = '/content/drive/MyDrive/á„ƒá…¦á„‹á…µá„á…¥ á„‡á…®á†«á„‰á…¥á†¨ á„†á…µá†¾ á„‰á…µá„€á…¡á†¨á„’á…ª/1. á„á…µá„‹á…¯á„ƒá…³ á„á…®á„á…®á†¯/image/protein_image.png'\n","    # wordcloud ê°ì²´ì˜ to_file() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n","    wordcloud.to_file(save_path)\n","    print(f\"ì´ë¯¸ì§€ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}\")\n","\n","# ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"DKqpD5SjCDQZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ê°ì„± ë¶„ì„ ë° ë¬¸ì„œ ìš”ì•½"],"metadata":{"id":"KDDsAU1_CM6z"}},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","def split_reviews_by_language(file_path, english_start_row, english_end_row):\n","    \"\"\"\n","    ì „ì²´ ë¦¬ë·° ë°ì´í„°ë¥¼ ë°›ì•„ ì˜ì–´ì™€ í•œê¸€ ë¦¬ë·°ë¡œ ë¶„ë¦¬í•˜ì—¬ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜.\n","\n","    Args:\n","        file_path (str): ì „ì²´ ë¦¬ë·° ë°ì´í„°ê°€ ë‹´ê¸´ CSV íŒŒì¼ ê²½ë¡œ.\n","        english_start_row (int): ì˜ì–´ ë¦¬ë·°ì˜ ì‹œì‘ í–‰ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘).\n","        english_end_row (int): ì˜ì–´ ë¦¬ë·°ì˜ ë í–‰ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘).\n","    \"\"\"\n","    try:\n","        # 1. ì „ì²´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n","        df = pd.read_csv(file_path)\n","        print(f\"âœ”ï¸ ì „ì²´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í–‰\")\n","\n","        # 2. ì˜ì–´ ë¦¬ë·°ë¥¼ ìŠ¬ë¼ì´ì‹±í•©ë‹ˆë‹¤.\n","        # pandasëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ í–‰ ë²ˆí˜¸ì—ì„œ 1ì„ ë¹¼ì„œ ì¸ë±ìŠ¤ì— ë§ì¶¥ë‹ˆë‹¤.\n","        english_df = df.iloc[english_start_row - 1 : english_end_row]\n","        print(f\"âœ”ï¸ ì˜ì–´ ë¦¬ë·° ì¶”ì¶œ ì™„ë£Œ: {len(english_df)}ê°œ í–‰\")\n","\n","        # 3. í•œê¸€ ë¦¬ë·°ë¥¼ ìŠ¬ë¼ì´ì‹±í•©ë‹ˆë‹¤. (ì˜ì–´ ë¦¬ë·°ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€)\n","        korean_df = df.drop(index=range(english_start_row - 1, english_end_row))\n","        print(f\"âœ”ï¸ í•œê¸€ ë¦¬ë·° ì¶”ì¶œ ì™„ë£Œ: {len(korean_df)}ê°œ í–‰\")\n","\n","        # 4. ê° DataFrameì„ ìƒˆë¡œìš´ CSV íŒŒì¼ë¡œ ì €ì¥í•  ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n","        output_folder = '/content/drive/MyDrive/ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”/1. í‚¤ì›Œë“œ ì¶”ì¶œ'\n","        if not os.path.exists(output_folder):\n","            os.makedirs(output_folder)\n","            print(f\"'{output_folder}' í´ë”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","\n","        # 5. ê° DataFrameì„ ìƒˆë¡œìš´ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n","        english_file_path = os.path.join(output_folder, 'ì˜ì–´_ë¦¬ë·°_ë°ì´í„°.csv')\n","        korean_file_path = os.path.join(output_folder, 'í•œê¸€_ë¦¬ë·°_ë°ì´í„°.csv')\n","\n","        english_df.to_csv(english_file_path, index=False)\n","        korean_df.to_csv(korean_file_path, index=False)\n","\n","        print(\"\\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n","        print(f\" 'ì˜ì–´_ë¦¬ë·°_ë°ì´í„°.csv' íŒŒì¼ê³¼ 'í•œê¸€_ë¦¬ë·°_ë°ì´í„°.csv' íŒŒì¼ì´ '{output_folder}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","\n","    except FileNotFoundError:\n","        print(f\"ì˜¤ë¥˜: {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","    except Exception as e:\n","        print(f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n","\n","# ì‹¤í–‰ ì½”ë“œ\n","file_path = '/content/drive/MyDrive/á„ƒá…¦á„‹á…µá„á…¥ á„‰á…®á„Œá…µá†¸/3. á„Œá…¥á†«á„á…¥á„…á…µX á„ƒá…¦á„‹á…µá„á…¥ á„á…©á†¼á„’á…¡á†¸/á„á…©á†¼á„’á…¡á†¸á„ƒá…¦á„‹á…µá„á…¥(á„€á…¡á†«á„ƒá…¡á†«á„’á…¡á†«_á„á…¦á†¨á„‰á…³á„á…³_á„Œá…¥á†«á„á…¥á„…á…µ).csv'\n","english_start_row = 67849\n","english_end_row = 130700\n","\n","split_reviews_by_language(file_path, english_start_row, english_end_row)"],"metadata":{"id":"mNML1ukFCN5Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","# 1. íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•´ ì£¼ì„¸ìš”.\n","file_path = '/content/drive/MyDrive/ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”/1. í‚¤ì›Œë“œ ì¶”ì¶œ/ì˜ì–´_ë¦¬ë·°_ë°ì´í„°.csv'\n","\n","try:\n","    df = pd.read_csv(file_path)\n","\n","    # 2. ì´ í–‰ ê°œìˆ˜ì™€ ì¤‘ê°„ì  ê³„ì‚°\n","    total_rows = len(df)\n","    split_point = total_rows // 2\n","\n","    print(f\"âœ”ï¸ ë°ì´í„°ì˜ ì´ í–‰ ê°œìˆ˜: {total_rows}\")\n","    print(f\"âœ”ï¸ ë°ì´í„°ë¥¼ ë‚˜ëˆŒ ì¤‘ê°„ì : {split_point}\")\n","\n","    # 3. ë°ì´í„°í”„ë ˆì„ì„ ë‘ ê°œë¡œ ë¶„í• \n","    df_part1 = df.iloc[:split_point].copy()\n","    df_part2 = df.iloc[split_point:].copy()\n","\n","    # 4. ì›í•˜ëŠ” ê²½ë¡œì™€ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ê°ê° ì €ì¥í•˜ê¸°\n","    output_folder = '/content/drive/MyDrive/á„ƒá…¦á„‹á…µá„á…¥ á„‡á…®á†«á„‰á…¥á†¨ á„†á…µá†¾ á„‰á…µá„€á…¡á†¨á„’á…ª/1. á„á…µá„‹á…¯á„ƒá…³ á„á…®á„á…®á†¯'\n","\n","    # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±\n","    os.makedirs(output_folder, exist_ok=True)\n","\n","    # ì²« ë²ˆì§¸ íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° ì €ì¥\n","    output_path1 = os.path.join(output_folder, 'ì˜ì–´_ë¦¬ë·°_ë°ì´í„°_part1.csv')\n","    df_part1.to_csv(output_path1, index=False, encoding='utf-8-sig')\n","\n","    # ë‘ ë²ˆì§¸ íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° ì €ì¥\n","    output_path2 = os.path.join(output_folder, 'ì˜ì–´_ë¦¬ë·°_ë°ì´í„°_part2.csv')\n","    df_part2.to_csv(output_path2, index=False, encoding='utf-8-sig')\n","\n","    print(f\"\\nğŸ‰ íŒŒì¼ì´ ì•„ë˜ ë‘ ê²½ë¡œì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n","    print(f\"- ì²« ë²ˆì§¸ íŒŒì¼: '{output_path1}'\")\n","    print(f\"- ë‘ ë²ˆì§¸ íŒŒì¼: '{output_path2}'\")\n","\n","except FileNotFoundError:\n","    print(f\"ì˜¤ë¥˜: '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")"],"metadata":{"id":"syyYU4fTCTvk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### í•œê¸€ ë¦¬ë·°"],"metadata":{"id":"U7qeXJoCCXON"}},{"cell_type":"code","source":["\n","def chunk_text(reviews_list, max_tokens=500):\n","    \"\"\"ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ìµœëŒ€ í† í° ìˆ˜ì— ë§ê²Œ í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ë¡œ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜\"\"\"\n","    text_to_chunk = \" \".join(reviews_list)\n","    chunks = []\n","    current_chunk = \"\"\n","    current_tokens = 0\n","    sentences = text_to_chunk.split('.')\n","    for sentence in sentences:\n","        sentence = sentence.strip()\n","        if not sentence: continue\n","        sentence_tokens = len(sentence.split())\n","        if current_tokens + sentence_tokens > max_tokens:\n","            chunks.append(current_chunk.strip())\n","            current_chunk = sentence + \". \"\n","            current_tokens = sentence_tokens\n","        else:\n","            current_chunk += sentence + \". \"\n","            current_tokens += sentence_tokens\n","    if current_chunk: chunks.append(current_chunk.strip())\n","    return chunks\n","\n","def summarize_korean(text, summarizer):\n","    \"\"\"í•œê¸€ ë¦¬ë·°ë§Œ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜\"\"\"\n","    try:\n","        return summarizer(text, max_length=150, min_length=30)[0]['summary_text']\n","    except Exception as e:\n","        return f\"ìš”ì•½ ì‹¤íŒ¨: {e}\"\n","\n","def main():\n","    print(\"--- Hugging Face ê¸°ë°˜ í•œê¸€ ë¦¬ë·° ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ì‹œì‘ ---\")\n","\n","    # 1. í•œê¸€ ë¦¬ë·° ë°ì´í„° íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. (ê²½ë¡œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”)\n","    file_path = '/content/drive/MyDrive/ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”/1. í‚¤ì›Œë“œ ì¶”ì¶œ/í•œê¸€_ë¦¬ë·°_ë°ì´í„°.csv'\n","    try:\n","        df = pd.read_csv(file_path)\n","    except FileNotFoundError:\n","        print(f\"ì˜¤ë¥˜: {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n","        return\n","\n","    # 2. ë¶„ì„í•  ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n","    categories = ['ë‹¨ë°±ì§ˆ', 'ë¹„íƒ€ë¯¼', 'ì•„ì—°', 'ì˜¤ë©”ê°€3', 'ìœ ì‚°ê· ', 'ë‹¨ë°±ì§ˆ ì‰ì´í¬', 'ë©€í‹°ë¹„íƒ€ë¯¼']\n","\n","    # 3. ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n","    print(\"\\n[1ë‹¨ê³„] ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...\")\n","    try:\n","        classifier = pipeline(\n","            \"sentiment-analysis\",\n","            model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","            tokenizer=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","            truncation=True,\n","            max_length=500\n","        )\n","    except Exception as e:\n","        print(f\"ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n","        return\n","\n","    # 4. í•œê¸€ ìš”ì•½ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n","    print(\"[2ë‹¨ê³„] í•œê¸€ ìš”ì•½ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...\")\n","    try:\n","        korean_summarizer = pipeline(\n","            \"summarization\",\n","            model=\"lcw99/t5-base-korean-text-summary\"\n","        )\n","    except Exception as e:\n","        print(f\"í•œê¸€ ìš”ì•½ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n","        return\n","\n","    # ìš”ì•½ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n","    summary_results = []\n","\n","    # 5. ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ë£¨í”„ ì‹œì‘\n","    for category in categories:\n","        print(f\"\\n--- {category} ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì‹œì‘ ---\")\n","\n","        # 5-1. í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ë¦¬ë·°ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n","        category_df = df[df['ì¹´í…Œê³ ë¦¬'] == category].copy()\n","\n","        if category_df.empty:\n","            print(\"í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.\")\n","            continue\n","\n","        # 5-2. ê°ì„± ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n","        print(\"ê°ì„± ë¶„ì„ ì¤‘...\")\n","        category_df['sentiment'] = category_df['ë¦¬ë·°'].apply(\n","            lambda review: classifier(review)[0]['label'] if isinstance(review, str) else 'neutral'\n","        )\n","        print(\"ë¶„ì„ ì™„ë£Œ.\")\n","\n","        # 5-3. ê¸ì •/ë¶€ì • ë¦¬ë·°ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.\n","        positive_reviews = category_df[category_df['sentiment'] == 'positive']['ë¦¬ë·°'].tolist()\n","        negative_reviews = category_df[category_df['sentiment'] == 'negative']['ë¦¬ë·°'].tolist()\n","\n","        # 5-4. ê¸ì • ë¦¬ë·° ìš”ì•½\n","        if positive_reviews:\n","            print(f\"âœ”ï¸ ê¸ì • ë¦¬ë·° ({len(positive_reviews)}ê°œ) ìš”ì•½\")\n","            positive_chunks = chunk_text(positive_reviews)\n","            for i, chunk in enumerate(positive_chunks):\n","                summary = summarize_korean(chunk, korean_summarizer)\n","                # ìš”ì•½ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n","                summary_results.append({\n","                    'category': category,\n","                    'sentiment': 'positive',\n","                    'summary': summary\n","                })\n","\n","        # 5-5. ë¶€ì • ë¦¬ë·° ìš”ì•½\n","        if negative_reviews:\n","            print(f\"âœ”ï¸ ë¶€ì • ë¦¬ë·° ({len(negative_reviews)}ê°œ) ìš”ì•½\")\n","            negative_chunks = chunk_text(negative_reviews)\n","            for i, chunk in enumerate(negative_chunks):\n","                summary = summarize_korean(chunk, korean_summarizer)\n","                # ìš”ì•½ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n","                summary_results.append({\n","                    'category': category,\n","                    'sentiment': 'negative',\n","                    'summary': summary\n","                })\n","\n","    # 6. ìš”ì•½ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n","    print(\"\\n--- ë¶„ì„ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. ---\")\n","    output_folder = '/content/drive/MyDrive/á„ƒá…¦á„‹á…µá„á…¥ á„‡á…®á†«á„‰á…¥á†¨ á„†á…µá†¾ á„‰á…µá„€á…¡á†¨á„’á…ª/1. á„á…µá„‹á…¯á„ƒá…³ á„á…®á„á…®á†¯/á„…á…µá„‡á…² á„€á…§á†¯á„€á…ª'\n","    output_file = 'í•œê¸€_ë¦¬ë·°_ìš”ì•½_ê²°ê³¼.csv'\n","    output_path = os.path.join(output_folder, output_file)\n","\n","    if summary_results:\n","        summary_df = pd.DataFrame(summary_results)\n","        summary_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n","        print(f\"ğŸ‰ ìš”ì•½ ê²°ê³¼ê°€ '{output_path}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n","    else:\n","        print(\"ì €ì¥í•  ìš”ì•½ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n","\n","\n","if __name__ == \"__main__\":\n","    # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ë‹¤ë©´ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•´ ì„¤ì¹˜í•˜ì„¸ìš”.\n","    # !pip install transformers pandas torch\n","    main()\n"],"metadata":{"id":"DQ9kegYJCW2i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### ì˜ì–´ ë¦¬ë·°"],"metadata":{"id":"Xh4ryrd8ClEo"}},{"cell_type":"code","source":["def chunk_text(reviews_list, max_tokens=300):\n","    \"\"\"ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ìµœëŒ€ í† í° ìˆ˜ì— ë§ê²Œ í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ë¡œ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜\"\"\"\n","    text_to_chunk = \" \".join(reviews_list)\n","    chunks = []\n","    current_chunk = \"\"\n","    current_tokens = 0\n","    # ë¬¸ì¥ ë¶„ë¦¬ë¥¼ ìœ„í•œ ì •ê·œì‹\n","    sentences = re.split(r'(?<!\\.\\w)(?<![A-Z][a-z])(?<=\\.|\\?|\\!)\\s', text_to_chunk)\n","    for sentence in sentences:\n","        sentence = sentence.strip()\n","        if not sentence: continue\n","        sentence_tokens = len(sentence.split())\n","        if current_tokens + sentence_tokens > max_tokens:\n","            chunks.append(current_chunk.strip())\n","            current_chunk = sentence + \" \"\n","            current_tokens = sentence_tokens\n","        else:\n","            current_chunk += sentence + \" \"\n","            current_tokens += sentence_tokens\n","    if current_chunk: chunks.append(current_chunk.strip())\n","    return chunks\n","\n","def summarize_english(text, summarizer):\n","    \"\"\"ì˜ì–´ ë¦¬ë·°ë§Œ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜\"\"\"\n","    try:\n","        # ëª¨ë¸ì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´ë¥¼ ê³ ë ¤í•´ í…ìŠ¤íŠ¸ë¥¼ ìë¦…ë‹ˆë‹¤.\n","        max_model_length = 512\n","        if len(text.split()) > max_model_length:\n","            text = \" \".join(text.split()[:max_model_length])\n","\n","        return summarizer(text, max_length=150, min_length=30)[0]['summary_text']\n","    except Exception as e:\n","        return f\"ìš”ì•½ ì‹¤íŒ¨: {e}\"\n","\n","def main():\n","    print(\"--- Hugging Face ê¸°ë°˜ ê³ ìœ ë²ˆí˜¸ & ì¹´í…Œê³ ë¦¬ë³„ ê°ì„± ë¶„ì„ ë° ìš”ì•½ ---\")\n","\n","    # 1. ì˜ì–´ ë¦¬ë·° ë°ì´í„° íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. (ê²½ë¡œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”)\n","    file_path = '/content/drive/MyDrive/á„ƒá…¦á„‹á…µá„á…¥ á„‡á…®á†«á„‰á…¥á†¨ á„†á…µá†¾ á„‰á…µá„€á…¡á†¨á„’á…ª/1. á„á…µá„‹á…¯á„ƒá…³ á„á…®á„á…®á†¯/á„‹á…§á†¼á„‹á…¥_á„…á…µá„‡á…²_á„ƒá…¦á„‹á…µá„á…¥_part1.csv'\n","    try:\n","        df = pd.read_csv(file_path)\n","        # í•„ìš”í•œ ì—´ì´ ìˆëŠ”ì§€ í™•ì¸\n","        if 'ê³ ìœ ë²ˆí˜¸' not in df.columns or 'ë¦¬ë·°' not in df.columns or 'ì¹´í…Œê³ ë¦¬' not in df.columns:\n","            print(\"ì˜¤ë¥˜: ë°ì´í„°í”„ë ˆì„ì— 'ê³ ìœ ë²ˆí˜¸', 'ì¹´í…Œê³ ë¦¬' ë˜ëŠ” 'ë¦¬ë·°' ì—´ì´ ì—†ìŠµë‹ˆë‹¤. ì—´ ì´ë¦„ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.\")\n","            return\n","    except FileNotFoundError:\n","        print(f\"ì˜¤ë¥˜: {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n","        return\n","\n","    # ë¦¬ë·° ì—´ì˜ ë¹ˆ ê°’ì„ ëª¨ë‘ ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›Œì¤ë‹ˆë‹¤.\n","    df['ë¦¬ë·°'] = df['ë¦¬ë·°'].fillna('')\n","\n","    # ë¦¬ë·° ì—´ì˜ ëª¨ë“  ê°’ì„ ë¬¸ìì—´ íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n","    df['ë¦¬ë·°'] = df['ë¦¬ë·°'].astype(str)\n","\n","    # 2. ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n","    print(\"\\n[1ë‹¨ê³„] ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...\")\n","    try:\n","        classifier = pipeline(\n","            \"sentiment-analysis\",\n","            model=\"AnkitAI/reviews-roberta-base-sentiment-analysis\",\n","            tokenizer=\"AnkitAI/reviews-roberta-base-sentiment-analysis\",\n","            truncation=True,\n","            max_length=512\n","        )\n","    except Exception as e:\n","        print(f\"ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n","        return\n","\n","    # 3. ì˜ì–´ ìš”ì•½ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n","    print(\"[2ë‹¨ê³„] ì˜ì–´ ìš”ì•½ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...\")\n","    try:\n","        english_summarizer = pipeline(\n","            \"summarization\",\n","            model=\"mabrouk/amazon-review-summarizer-bart\"\n","        )\n","    except Exception as e:\n","        print(f\"ì˜ì–´ ìš”ì•½ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n","        return\n","\n","    # 4. ë¦¬ë·°ì— ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n","    print(\"\\n[3ë‹¨ê³„] ë¦¬ë·° ê°ì„±ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...\")\n","    # ë¦¬ë·° ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n","    reviews_list = df['ë¦¬ë·°'].tolist()\n","\n","    # ë¦¬ìŠ¤íŠ¸ ì „ì²´ë¥¼ í•œ ë²ˆì— ëª¨ë¸ì— ì „ë‹¬\n","    results = classifier(reviews_list)\n","\n","    # ê²°ê³¼ë§Œ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€ (ëª¨ë¸ì´ ì§ì ‘ ì¤‘ë¦½ì„ ë¶„ë¥˜í•˜ë¯€ë¡œ ë¡œì§ ë‹¨ìˆœí™”)\n","    def get_sentiment(result):\n","        # 'AnkitAI' ëª¨ë¸ì€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ë¥¼ í•˜ë‚˜ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.\n","        label = result['label']\n","\n","        # ëª¨ë¸ ë¼ë²¨(POSITIVE, NEGATIVE)ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n","        return label\n","\n","    # ìƒˆë¡œ ì •ì˜í•œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì„± ì—´ì„ ì±„ì›ë‹ˆë‹¤.\n","    # ê²°ê³¼ëŠ” [{'label': 'POSITIVE', 'score': 0.99}, ...]ì™€ ê°™ì€ í˜•íƒœì…ë‹ˆë‹¤.\n","    df['sentiment'] = [get_sentiment(result) for result in results]\n","    print(\"ë¶„ì„ ì™„ë£Œ.\")\n","\n","    # 5. ê³ ìœ ë²ˆí˜¸, ì¹´í…Œê³ ë¦¬, ê°ì„±ë³„ë¡œ ê·¸ë£¹í™”í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤.\n","    print(\"\\n[4ë‹¨ê³„] ê³ ìœ ë²ˆí˜¸, ì¹´í…Œê³ ë¦¬, ê°ì„±ë³„ë¡œ ë¦¬ë·°ë¥¼ ìš”ì•½í•˜ê³  ìˆìŠµë‹ˆë‹¤...\")\n","\n","    # 'ê³ ìœ ë²ˆí˜¸', 'ì¹´í…Œê³ ë¦¬', 'sentiment'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.\n","    grouped_df = df.groupby(['ê³ ìœ ë²ˆí˜¸', 'ì¹´í…Œê³ ë¦¬', 'sentiment'])['ë¦¬ë·°'].apply(lambda x: ' '.join(x)).reset_index()\n","\n","    # ìš”ì•½ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n","    summary_results = []\n","\n","    for index, row in grouped_df.iterrows():\n","        unique_id = row['ê³ ìœ ë²ˆí˜¸']\n","        category = row['ì¹´í…Œê³ ë¦¬']\n","        sentiment = row['sentiment']\n","        reviews_text = row['ë¦¬ë·°']\n","\n","        print(f\"--- ê³ ìœ ë²ˆí˜¸: {unique_id}, ì¹´í…Œê³ ë¦¬: {category}, ê°ì„±: {sentiment} ìš”ì•½ ì‹œì‘ ---\")\n","\n","        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê³  ìš”ì•½\n","        chunks = chunk_text([reviews_text])\n","        for chunk in chunks:\n","            summary = summarize_english(chunk, english_summarizer)\n","            summary_results.append({\n","                'ê³ ìœ ë²ˆí˜¸': unique_id,\n","                'ì¹´í…Œê³ ë¦¬': category,\n","                'sentiment': sentiment,\n","                'summary': summary\n","            })\n","\n","    # 6. ìš”ì•½ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n","    print(\"\\n--- ë¶„ì„ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. ---\")\n","    output_folder = '/content/drive/MyDrive/ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”/1. í‚¤ì›Œë“œ ì¶”ì¶œ/ë¦¬ë·° ê²°ê³¼'\n","    output_file = 'ì˜ì–´ë¦¬ë·°_ê°ì„±_ìš”ì•½_ê²°ê³¼_part1.csv'\n","    output_path = os.path.join(output_folder, output_file)\n","\n","    if summary_results:\n","        summary_df = pd.DataFrame(summary_results)\n","        # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±\n","        os.makedirs(output_folder, exist_ok=True)\n","        summary_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n","        print(f\"ğŸ‰ ìš”ì•½ ê²°ê³¼ê°€ '{output_path}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n","    else:\n","        print(\"ì €ì¥í•  ìš”ì•½ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n","\n","\n","if __name__ == \"__main__\":\n","    # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ë‹¤ë©´ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•´ ì„¤ì¹˜í•˜ì„¸ìš”.\n","    # !pip install transformers pandas torch\n","    main()"],"metadata":{"id":"kHxmAEPTClw8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GCP gemini API ì´ìš©í•œ ë¦¬ë·°ìš”ì•½"],"metadata":{"id":"B5eCU0e0CqJH"}},{"cell_type":"code","source":["# llm (genai)\n","!pip install llama-index-llms-google-genai llama-index\n","\n","# ì„ë² ë”© (genai)\n","!pip install llama-index-embeddings-google-genai\n","\n","!pip install --upgrade google-generativeai\n","\n","### í•„ìš”í•œ í•¨ìˆ˜ ì„í´íŠ¸\n","import os\n","from dotenv import load_dotenv\n","# from llama_index.llms.openai import OpenAI\n","from llama_index.llms.google_genai import GoogleGenAI\n","# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","from llama_index.core import Settings, Document, VectorStoreIndex\n","from llama_index.core.tools import QueryEngineTool, ToolMetadata\n","from llama_index.core.agent import ReActAgent\n","import tqdm\n","import pandas as pd\n","\n","### df_grouped ë°ì´í„° í”„ë ˆì„ ì„í´íŠ¸\n","\n","import pandas as pd\n","\n","file_path = '/content/drive/MyDrive/á„‹á…µá†·á„‡á…¦á„ƒá…µá†¼/GCP gemini API á„’á…ªá†¯á„‹á…­á†¼/1. á„…á…µá„‡á…²á„á…©á†¼á„’á…¡á†¸.csv'\n","\n","df_grouped = pd.read_csv(file_path)\n","\n","# df_grouped\n","\n","### í•„ìš”í•œ í•¨ìˆ˜ ì„í´íŠ¸\n","import os\n","import json\n","import pandas\n","import pickle\n","from dotenv import load_dotenv\n","import google.generativeai as generativeai\n","import pandas as pd\n","import random\n","import time\n","import re\n","\n","### .env file ë¡œë“œ\n","load_dotenv('/content/drive/MyDrive/á„‹á…µá†·á„‡á…¦á„ƒá…µá†¼/GCP gemini API á„’á…ªá†¯á„‹á…­á†¼/.env')\n","\n","### gemini key ë¶ˆëŸ¬ì˜¤ê¸°ê¸°\n","GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n","generativeai.configure(api_key=GOOGLE_API_KEY)\n","\n","## API í‚¤ í™•ì¸\n","\n","import google.generativeai as generativeai\n","print(os.getenv(\"GOOGLE_API_KEY\"))  # ì˜¬ë°”ë¥¸ ê°’ì´ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸\n","\n","### ê°ì„± ë¶„ì„ ë° ê·¼ê±°ê°€ ë˜ëŠ” keyword ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜\n","def analyze_review(text_input):\n","    # prompt ìƒì„±\n","    prompt = f\"\"\"\n","    ë‹¤ìŒ í…ìŠ¤íŠ¸ëŠ” ê±´ê°•ê¸°ëŠ¥ì‹í’ˆì— ê´€í•œ ì†Œë¹„ì ë¦¬ë·°ì…ë‹ˆë‹¤. ì œí’ˆ í•˜ë‚˜ë‹¹ ëª¨ë“  ë¦¬ë·°ê°€ '/'ë¡œ êµ¬ë¶„ë˜ì–´ì ¸ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n","    í•´ë‹¹ ë¦¬ë·°ì˜ ë‚´ìš©ì—ì„œ ê¸ì •ì¸ ë‚´ìš©, ë¶€ì •ì¸ ë‚´ìš©, ê·¸ë¦¬ê³  ì¤‘ë¦½ì¸ ë‚´ìš©ì„ ë¶„ë¥˜ë¥¼ í•˜ê³ , ê° ê°ì„±ë³„ë¡œ ë¦¬ë·°ë¥¼ ìš”ì•½í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì œì‹œí•´ ì£¼ì„¸ìš”.\n","    ê° ê°ì„±ë³„ ë¦¬ë·° ìš”ì•½ì€ ë§¥ë½ì´ ë¹„ìŠ·í•œ ê²ƒë§Œ ë‚¨ê¸°ê³  ìš”ì•½ì„ í•˜ì—¬, ê° ê°ì„±ë³„ ë¦¬ë·°ì˜ ê¸€ì ìˆ˜ê°€ 300ìë¥¼ ë„˜ì§€ ì•Šë„ë¡ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n","    ê·¸ë¦¬ê³  ë¦¬ë·° ìš”ì•½í•  ë•Œ ì—†ëŠ” ë‚´ìš©ì„ ì°½ì¡°í•˜ë©´ ì•ˆë˜ê³ , ë¦¬ë·° ë‚´ìš©ì— 100% ì¶©ì‹¤í•˜ê²Œ ìš”ì•½í•´ì•¼ í•´ì•¼ í•©ë‹ˆë‹¤.\n","    JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì•„ë˜ í˜•ì‹ ì™¸ì˜ ì„¤ëª… ë¬¸êµ¬ëŠ” ì ˆëŒ€ ì“°ì§€ ë§ˆì„¸ìš”.\n","\n","    í…ìŠ¤íŠ¸: {text_input}\n","\n","    ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:\n","    {{\n","      \"ì •í™•ë„\": 85,\n","      \"ê°ì„±ë³„ ë¦¬ë·° ìš”ì•½\": {{\n","        \"ê¸ì •\": [\"ê¸ì • ë¦¬ë·° ìš”ì•½\"],\n","        \"ë¶€ì •\": [\"ë¶€ì • ë¦¬ë·° ìš”ì•½\"],\n","        \"ì¤‘ë¦½\": [\"ì¤‘ë¦½ ë¦¬ë·° ìš”ì•½\"]\n","      }}\n","    }}\n","    \"\"\"\n","\n","\n","    try:\n","        # í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼ ìƒì„±\n","        model = generativeai.GenerativeModel(\"gemini-2.5-pro\")\n","\n","        # generation_config ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ temperatureë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n","        # temperature ê°’ì€ 0.0 (ê°€ì¥ ë³´ìˆ˜ì )ì—ì„œ 1.0 (ê°€ì¥ ì°½ì˜ì ) ì‚¬ì´ë¡œ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","        generation_config = generativeai.types.GenerationConfig(\n","            temperature=0.0,  # ì›í•˜ëŠ” temperature ê°’ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš” (ì˜ˆ: 0.2, 0.5, 0.9 ë“±)\n","            top_p=0.9,\n","            top_k=50\n","        )\n","\n","        response = model.generate_content(\n","            contents=[prompt],\n","            generation_config=generation_config # ì—¬ê¸°ì— generation_configë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n","        )\n","\n","\n","        result_str = response.text\n","        if result_str:\n","            print(f'ê²°ê³¼ : {result_str}')\n","\n","            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ\n","            match = re.search(r\"\\{[\\s\\S]*\\}\", result_str)\n","            if match:\n","                json_text = match.group(0).strip()\n","                try:\n","                    parsed_json = json.loads(json_text)\n","                    return parsed_json\n","                except json.JSONDecodeError:\n","                    print(\"JSON ë””ì½”ë”© ì‹¤íŒ¨. ì¶”ì¶œëœ ë‚´ìš©:\", json_text)\n","                    return None\n","            else:\n","                print(\"JSON íŒ¨í„´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n","                return None\n","\n","\n","    except Exception as e:\n","        print(f'API í˜¸ì¶œ ì˜¤ë¥˜ : {e}')\n","        return None\n","\n","\n","\n","from tqdm.notebook import tqdm  # Colab/Jupyterìš© ì§„í–‰ë°”\n","\n","### ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ ì •ì˜ (tqdm ì ìš©)\n","def process_multiple_texts(text_list):\n","    results = {}\n","    for i, text_input in enumerate(tqdm(text_list, desc=\"ë¦¬ë·° ì²˜ë¦¬ ì§„í–‰ë¥ \")):\n","        retry_count = 0\n","        max_retries = 5  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜\n","        wait_time = 20  # ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ (ì´ˆ)\n","\n","        while retry_count < max_retries:\n","            print(f\"í…ìŠ¤íŠ¸ {i+1} ì²˜ë¦¬ ì‹œë„ {retry_count + 1}...\")\n","            result = analyze_review(text_input)\n","            if result:\n","            #     results[f\"í…ìŠ¤íŠ¸ {i+1}\"] = result\n","            #     break  # ì„±ê³µ ì‹œ ë£¨í”„ ì¢…ë£Œ\n","            # else:\n","            #     retry_count += 1\n","            #     wait_time = wait_time * 2 + random.uniform(0, 1)  # ì§€ìˆ˜ ë°±ì˜¤í”„ + ì•½ê°„ì˜ ì„ì˜ ì‹œê°„ ì¶”ê°€\n","            #     print(f\"API ìš”ì²­ ì‹¤íŒ¨. {wait_time:.2f}ì´ˆ í›„ ì¬ì‹œë„...\")\n","            #     time.sleep(wait_time)\n","\n","                result = analyze_review(text_input)\n","\n","                # ì„±ê³µ ì¡°ê±´ì„ \"ì˜¬ë°”ë¥¸ ë”•ì…”ë„ˆë¦¬(JSON íŒŒì‹± ì„±ê³µ)\"ë¡œ í•œì •\n","                if isinstance(result, dict):\n","                    results[f\"í…ìŠ¤íŠ¸ {i+1}\"] = result\n","                    break  # ì„±ê³µ ì‹œ ë£¨í”„ ì¢…ë£Œ\n","                else:\n","                    retry_count += 1\n","                    wait_time = wait_time * 2 + random.uniform(0, 1)\n","                    print(f\"API ìš”ì²­ ë˜ëŠ” JSON íŒŒì‹± ì‹¤íŒ¨. {wait_time:.2f}ì´ˆ í›„ ì¬ì‹œë„...\")\n","                    time.sleep(wait_time)\n","\n","        else:  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ì‹œ\n","            results[f\"í…ìŠ¤íŠ¸ {i+1}\"] = \"ê°ì • í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ (ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼)\"\n","\n","    return results\n","\n","\n","\n","\n","# ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì²˜ë¦¬\n","all_results = process_multiple_texts(review_text)\n","\n","# ìµœì¢… ê²°ê³¼ ì¶œë ¥\n","# print(all_results)\n","\n","# ìµœì¢… ê²°ê³¼ë¥¼ dict ìë£Œ êµ¬ì¡°ë¡œ ë³€í™˜\n","with open('review_analysis.pkl', 'wb') as fw:\n","    pickle.dump(all_results, fw)\n","\n","# ì €ì¥ëœ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸°\n","with open('review_analysis.pkl', 'rb') as fr:\n","    loaded_data = pickle.load(fr)\n","\n","print(f'ë¦¬ë·° ë°ì´í„° ë¶„ì„ì˜ ê²°ê³¼ : \\n{loaded_data}')"],"metadata":{"id":"V6CHOL6VCtvu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ì²­í‚¹ ë° ì„ë² ë”©"],"metadata":{"id":"elNMd4IaC22Z"}},{"cell_type":"code","source":["# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n","!pip install pandas sentence_transformers --upgrade llama-index tqdm nltk llama-index-llms-google-genai llama-index-embeddings-huggingface\n","\n","# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í´íŠ¸\n","import pandas as pd\n","from sentence_transformers import SentenceTransformer\n","from llama_index.core.node_parser import SentenceSplitter # from llama_index.core.node_parser.text.sentence import SentenceSplitterì™€ ê°™ìŒ\n","from tqdm import tqdm\n","import ast\n","\n","# ë°ì´í„° í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°\n","df = pd.read_csv('/content/drive/MyDrive/á„‹á…µá†·á„‡á…¦á„ƒá…µá†¼/GCP gemini API á„’á…ªá†¯á„‹á…­á†¼/6.ë¦¬ë·°ë¶„ì„(ê°ì„±_ë¦¬ë·°ë§Œ ìˆëŠ” ë²„ì „)')\n","\n","# 1. CSV ë¶ˆëŸ¬ì˜¤ê¸° (ë¦¬ë·° í…ìŠ¤íŠ¸ê°€ 'ë¦¬ë·°' ì»¬ëŸ¼ì— ìˆë‹¤ê³  ê°€ì •)\n","df = pd.read_csv('/content/drive/MyDrive/á„‹á…µá†·á„‡á…¦á„ƒá…µá†¼/GCP gemini API á„’á…ªá†¯á„‹á…­á†¼/6.ë¦¬ë·°ë¶„ì„(ê°ì„±_ë¦¬ë·°ë§Œ ìˆëŠ” ë²„ì „)')\n","\n","# 2. ë¬¸ì¥ ë‹¨ìœ„ ì²­í‚¹ í•¨ìˆ˜ ì •ì˜\n","def chunk_text(text, chunk_size=200, chunk_overlap=50):\n","    splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n","    chunks = splitter.split_text(text)\n","    return chunks\n","\n","# 3. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ë¬´ë£Œ ë‹¤êµ­ì–´ ëª¨ë¸)\n","model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n","\n","# 4. ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„\n","rows = []\n","\n","# 5. ì „ì²´ ë¦¬ë·°ì— ëŒ€í•´ ì²˜ë¦¬ (ì²­í‚¹ + ì„ë² ë”©)\n","for idx, row in tqdm(df.iterrows(), total=len(df)):\n","    ê³ ìœ ë²ˆí˜¸ = row['ê³ ìœ ë²ˆí˜¸']\n","    ì œí’ˆëª… = row['ì œí’ˆëª…']\n","    ì¹´í…Œê³ ë¦¬ = row['ì¹´í…Œê³ ë¦¬']\n","    ìƒì„¸ì •ë³´ = row['ìƒì„¸ì •ë³´']\n","    ê¸ì •ë¦¬ë·° = row['ê¸ì •_ë¦¬ë·°']\n","    ë¶€ì •ë¦¬ë·° = row['ë¶€ì •_ë¦¬ë·°']\n","    ì¤‘ë¦½ë¦¬ë·° = row['ì¤‘ë¦½_ë¦¬ë·°']\n","\n","    # ê²°ì¸¡ì¹˜ë‚˜ ìˆ«ì ë¬¸ì œ í•´ê²°: ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ìš°ê³  ë¬¸ìì—´ ë³€í™˜\n","    if pd.isna(ê¸ì •ë¦¬ë·°):\n","        ê¸ì •ë¦¬ë·° = ''\n","    ê¸ì •ë¦¬ë·° = str(ê¸ì •ë¦¬ë·°)\n","\n","    if ê¸ì •ë¦¬ë·°.strip() == '':\n","        continue  # ë¹ˆ í…ìŠ¤íŠ¸ë©´ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ\n","\n","    chunks = chunk_text(ê¸ì •ë¦¬ë·°)\n","    for chunk in chunks:\n","        embedding = model.encode(chunk).tolist()\n","        rows.append({\n","            'ê³ ìœ ë²ˆí˜¸': ê³ ìœ ë²ˆí˜¸,\n","            'ì œí’ˆëª…' : ì œí’ˆëª…,\n","            'ì¹´í…Œê³ ë¦¬' : ì¹´í…Œê³ ë¦¬,\n","            'ìƒì„¸ì •ë³´' : ìƒì„¸ì •ë³´,\n","            'ê¸ì •ë¦¬ë·°' : ê¸ì •ë¦¬ë·°,\n","            'ë¶€ì •ë¦¬ë·°' : ë¶€ì •ë¦¬ë·°,\n","            'ì¤‘ë¦½ë¦¬ë·°' : ì¤‘ë¦½ë¦¬ë·°,\n","            'ê¸ì •_ì²­í¬': chunk,\n","            'ê¸ì •ë¦¬ë·°_ì„ë² ë”©': embedding\n","        })\n","\n","# 6. DataFrameìœ¼ë¡œ ë³€í™˜\n","result_df = pd.DataFrame(rows)\n","\n","# 7. ì„ë² ë”© ë²¡í„°ë¥¼ ë¬¸ìì—´ë¡œ ì €ì¥í•˜ê¸° ìœ„í•´ ë³€í™˜\n","result_df['ê¸ì •ë¦¬ë·°_ì„ë² ë”©'] = result_df['ê¸ì •ë¦¬ë·°_ì„ë² ë”©'].apply(lambda x: str(x))\n","\n","# 8. CSVë¡œ ì €ì¥\n","result_df.to_csv('/content/drive/MyDrive/á„‹á…µá†·á„‡á…¦á„ƒá…µá†¼/GCP gemini API á„’á…ªá†¯á„‹á…­á†¼/reviews_chunked_embeddings4.csv', index=False)\n","\n","print(\"ì²˜ë¦¬ ì™„ë£Œ! '/content/drive/MyDrive/á„‹á…µá†·á„‡á…¦á„ƒá…µá†¼/GCP gemini API á„’á…ªá†¯á„‹á…­á†¼/reviews_chunked_embeddings4.csv' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"],"metadata":{"id":"Ry48qSZLC46A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"],"metadata":{"id":"I68KQqALC7mR"}},{"cell_type":"code","source":["import pandas as pd\n","import ast\n","\n","file_path4 = '/content/drive/MyDrive/á„‹á…µá†·á„‡á…¦á„ƒá…µá†¼/GCP gemini API á„’á…ªá†¯á„‹á…­á†¼/reviews_chunked_embeddings4.csv'\n","\n","df = pd.read_csv(file_path4)\n","\n","# ë¬¸ìì—´ ì»¬ëŸ¼ -> ë¦¬ìŠ¤íŠ¸ ë³€í™˜\n","df['ê¸ì •ë¦¬ë·°_ì„ë² ë”©_int'] = df['ê¸ì •ë¦¬ë·°_ì„ë² ë”©'].apply(ast.literal_eval)\n","\n","print(type(df.loc[0, 'ê¸ì •ë¦¬ë·°_ì„ë² ë”©_int']))  # <class 'list'>\n","\n","condition_zinc = (df.loc[:,'ì¹´í…Œê³ ë¦¬']=='ì•„ì—°')\n","df_zinc = df.loc[condition_zinc,:]\n","\n","condition_vitamin = (df.loc[:,'ì¹´í…Œê³ ë¦¬']=='ë¹„íƒ€ë¯¼')\n","df_vitamin = df.loc[condition_vitamin,:]\n","\n","condition_probiotics = (df.loc[:,'ì¹´í…Œê³ ë¦¬']=='ìœ ì‚°ê· ')\n","df_probiotics = df.loc[condition_probiotics,:]\n","\n","condition_omega3 = (df.loc[:,'ì¹´í…Œê³ ë¦¬']=='ì˜¤ë©”ê°€3')\n","df_omega3 = df.loc[condition_omega3,:]\n","\n","condition_protein = (df.loc[:,'ì¹´í…Œê³ ë¦¬']=='ë‹¨ë°±ì§ˆ')\n","df_protein = df.loc[condition_protein,:]\n","\n","### ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n","import pandas as pd\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# ì¹´í…Œê³ ë¦¬ ì„ íƒ\n","category = input(\"ì›í•˜ëŠ” ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì¢…ë¥˜ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\\n\\n ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì¢…ë¥˜: ì•„ì—°, ë¹„íƒ€ë¯¼, ìœ ì‚°ê· , ì˜¤ë©”ê°€3, ë‹¨ë°±ì§ˆ ä¸­ \\t\")\n","\n","if category == 'ì•„ì—°':\n","    df = df_zinc\n","elif category == 'ë¹„íƒ€ë¯¼':\n","    df = df_vitamin\n","elif category == 'ìœ ì‚°ê· ':\n","    df = df_probiotics\n","elif category == 'ì˜¤ë©”ê°€3':\n","    df = df_omega3\n","elif category == 'ë‹¨ë°±ì§ˆ':\n","    df = df_protein\n","\n","# ilocê³¼ loc ê°™ê²Œ ë§Œë“¤ê¸°\n","df = df.reset_index(drop=True)\n","\n","\n","# 3. ê²€ìƒ‰í•  ì¿¼ë¦¬\n","# í•˜ë£¨ í•„ìš”í•œ ë¹„íƒ€ë¯¼ê³¼ ë¯¸ë„¤ë„ì´ ëª¨ë‘ ë“¤ì–´ìˆëŠ” ì œí’ˆ\n","query = input(\"ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸:\\t\") # í…ŒìŠ¤íŠ¸ìš© ë¬¸ì¥(ë°”ê¿”ì„œ í•´ë³´ë©´ ë¨)\n","\n","# 4. ì¿¼ë¦¬ ì„ë² ë”© (ë¦¬ë·° ì„ë² ë”© ëª¨ë¸ê³¼ ë™ì¼í•œ ëª¨ë¸ ì‚¬ìš©)\n","model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n","query_embedding = model.encode([query])\n","\n","# 5. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n","embeddings_matrix = np.array(df['ê¸ì •ë¦¬ë·°_ì„ë² ë”©_int'].tolist())\n","similarities = cosine_similarity(query_embedding, embeddings_matrix)[0]\n","\n","# 6. ìƒìœ„ 2ê°œ ë¬¸ì¥ ì¶œë ¥\n","top_n = 2\n","top_indices = similarities.argsort()[-top_n:][::-1]\n","unique_results = []\n","seen_ids = set()\n","\n","for idx in top_indices:\n","    product_id = df.iloc[idx]['ê³ ìœ ë²ˆí˜¸']\n","\n","    if product_id not in seen_ids:\n","        seen_ids.add(product_id)\n","        unique_results.append((idx, similarities[idx]))\n","\n","    if len(unique_results) == top_n:\n","        break\n","\n","# ì¶œë ¥\n","def result(unique_results):\n","    outputs = []\n","    for idx, score in unique_results:\n","        row = df.loc[idx]\n","        outputs.append(\n","            f\"\\nì œí’ˆëª…: {row['ì œí’ˆëª…']} | ê³ ìœ ë²ˆí˜¸: {row['ê³ ìœ ë²ˆí˜¸']} | ì¹´í…Œê³ ë¦¬: {row['ì¹´í…Œê³ ë¦¬']} \"\n","            f\"| ìƒì„¸ì •ë³´: {row['ìƒì„¸ì •ë³´']} \"\n","            f\"\\nê¸ì • ë¦¬ë·°: {row['ê¸ì •ë¦¬ë·°']} \\në¶€ì • ë¦¬ë·°: {row['ë¶€ì •ë¦¬ë·°']} \"\n","            f\"\\nì¤‘ë¦½ ë¦¬ë·°: {row['ì¤‘ë¦½ë¦¬ë·°']}\"\n","        )\n","        print(f\"\\nì¸ë±ìŠ¤: {idx} | ìœ ì‚¬ë„: {score:.4f} | ì œí’ˆëª…: {row['ì œí’ˆëª…']} | ê³ ìœ ë²ˆí˜¸: {row['ê³ ìœ ë²ˆí˜¸']} | ì¹´í…Œê³ ë¦¬: {row['ì¹´í…Œê³ ë¦¬']} \"\n","              f\"| ìƒì„¸ì •ë³´: {row['ìƒì„¸ì •ë³´']} \"\n","              f\"\\nê¸ì • ë¦¬ë·°: {row['ê¸ì •ë¦¬ë·°']}\"\n","             # f\"\\në¶€ì • ë¦¬ë·°: {row['ë¶€ì •ë¦¬ë·°']}\\nì¤‘ë¦½ ë¦¬ë·°: {row['ì¤‘ë¦½ë¦¬ë·°']}\"\n","        )\n","    return outputs\n","\n","product_result = result(unique_results)"],"metadata":{"id":"eT_vZUvwC6MB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# RAG êµ¬í˜„(LanghChain ê¸°ë°˜)"],"metadata":{"id":"A9sR-fkCCw7x"}},{"cell_type":"code","source":["from langchain.llms import HuggingFacePipeline\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.prompts import PromptTemplate\n","from langchain.vectorstores import Chroma\n","from langchain.chains import RetrievalQA\n","from langchain.document_loaders import PyPDFLoader, PyMuPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter"],"metadata":{"id":"J_wuwmBPCxxb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","file_path4 = '/content/drive/MyDrive/á„‹á…µá†·á„‡á…¦á„ƒá…µá†¼/GCP gemini API á„’á…ªá†¯á„‹á…­á†¼/reviews_chunked_embeddings4.csv'\n","\n","df = pd.read_csv(file_path4)"],"metadata":{"id":"6bs4H7RfDLbt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import ast\n","df['ê¸ì •ë¦¬ë·°_ì„ë² ë”©_int'] = df['ê¸ì •ë¦¬ë·°_ì„ë² ë”©'].apply(ast.literal_eval)\n","\n","condition_zinc = (df.loc[:,'ì¹´í…Œê³ ë¦¬']=='ì•„ì—°')\n","df_zinc = df.loc[condition_zinc,:]\n","\n","condition_vitamin = (df.loc[:,'ì¹´í…Œê³ ë¦¬']=='ë¹„íƒ€ë¯¼')\n","df_vitamin = df.loc[condition_vitamin,:]\n","\n","condition_probiotics = (df.loc[:,'ì¹´í…Œê³ ë¦¬']=='ìœ ì‚°ê· ')\n","df_probiotics = df.loc[condition_probiotics,:]\n","\n","condition_omega3 = (df.loc[:,'ì¹´í…Œê³ ë¦¬']=='ì˜¤ë©”ê°€3')\n","df_omega3 = df.loc[condition_omega3,:]\n","\n","condition_protein = (df.loc[:,'ì¹´í…Œê³ ë¦¬']=='ë‹¨ë°±ì§ˆ')\n","df_protein = df.loc[condition_protein,:]"],"metadata":{"id":"kGe936B0DM9F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### PDF Load"],"metadata":{"id":"FDDAIpaQDRW6"}},{"cell_type":"code","source":["import glob\n","\n","pdf_folder = \"/content/drive/MyDrive/RAG/PDF/*.pdf\"\n","pdf_files = glob.glob(pdf_folder)"],"metadata":{"id":"9PSVQLnmDOOv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","# ì²­í¬ ë¶„í• ê¸°\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size = 500,\n","    chunk_overlap = 50,\n",")\n","\n","# PDFë¬¸ì„œ ë¡œë“œ & ì²­í¬ ë¶„í• \n","all_docs = []\n","\n","for pdf_file in tqdm(pdf_files, desc = \"PDF Load :\"):\n","    loader = PyMuPDFLoader(pdf_file)\n","    docs = loader.load()\n","    all_docs.extend(docs)\n","\n","all_chunks = text_splitter.split_documents(all_docs)\n","\n","print(f\"ì´ ë¬¸ì„œ ìˆ˜: {len(all_chunks)}\")"],"metadata":{"id":"bvu5sMU_DS5A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs = []\n","for i, c in enumerate(all_chunks):\n","    docs.append({\n","        \"id\": f\"doc-{i}\",\n","        \"title\": c.metadata.get(\"source\", \"\"),  # PDF íŒŒì¼ëª…\n","        \"page\": c.metadata.get(\"page\", -1),    # í˜ì´ì§€ ë²ˆí˜¸\n","        \"text\": c.page_content                 # ì‹¤ì œ í…ìŠ¤íŠ¸\n","    })"],"metadata":{"id":"iPbmLP-9DVKx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### RAG"],"metadata":{"id":"jLDMWuNxDVj3"}},{"cell_type":"code","source":["import os, json, re, textwrap\n","from typing import List, Dict, Any, Tuple\n","\n","from sentence_transformers import SentenceTransformer, CrossEncoder\n","from sklearn.metrics.pairwise import cosine_similarity\n","from rank_bm25 import BM25Okapi\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","\n","# ==================================================\n","# 0. ë°ì´í„°/ì¹´í…Œê³ ë¦¬ ì„ íƒ\n","# ==================================================\n","category = input(\"ì›í•˜ëŠ” ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì¢…ë¥˜ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\\n\\n ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì¢…ë¥˜: ì•„ì—°, ë¹„íƒ€ë¯¼, ìœ ì‚°ê· , ì˜¤ë©”ê°€3, ë‹¨ë°±ì§ˆ ä¸­ \\t\")\n","\n","# ê° ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„°í”„ë ˆì„ì´ ìˆë‹¤ê³  ê°€ì •\n","if category == 'ì•„ì—°':\n","    df = df_zinc\n","elif category == 'ë¹„íƒ€ë¯¼':\n","    df = df_vitamin\n","elif category == 'ìœ ì‚°ê· ':\n","    df = df_probiotics\n","elif category == 'ì˜¤ë©”ê°€3':\n","    df = df_omega3\n","elif category == 'ë‹¨ë°±ì§ˆ':\n","    df = df_protein\n","else:\n","    raise ValueError(\"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤.\")\n","\n","df = df.reset_index(drop=True)\n","\n","# ==================================================\n","# 1. ë¦¬ë·° ê¸°ë°˜ ì¶”ì²œ (ì„ë² ë”© + ì½”ì‚¬ì¸ ìœ ì‚¬ë„)\n","# ==================================================\n","embed_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n","\n","def search_product_from_reviews(query: str, df: pd.DataFrame, top_n: int = 2):\n","    query_embedding = embed_model.encode([query])\n","    embeddings_matrix = np.array(df['ê¸ì •ë¦¬ë·°_ì„ë² ë”©_int'].tolist())\n","    similarities = cosine_similarity(query_embedding, embeddings_matrix)[0]\n","\n","    top_indices = similarities.argsort()[-top_n:][::-1]\n","    unique_results = []\n","    seen_ids = set()\n","\n","    for idx in top_indices:\n","        product_id = df.iloc[idx]['ê³ ìœ ë²ˆí˜¸']\n","        if product_id not in seen_ids:\n","            seen_ids.add(product_id)\n","            unique_results.append((idx, similarities[idx]))\n","        if len(unique_results) == top_n:\n","            break\n","\n","    return unique_results\n","\n","def product_result(unique_results, df: pd.DataFrame):\n","    outputs = []\n","    for idx, score in unique_results:\n","        row = df.loc[idx]\n","        outputs.append(f\"ì œí’ˆëª…: {row['ì œí’ˆëª…']}\")\n","    return outputs\n","\n","# ==================================================\n","# 2. ë¬¸ì„œ ê¸°ë°˜ ê²€ìƒ‰ (BM25 + Dense + RRF + Reranker)\n","# ==================================================\n","docs: List[Dict[str, Any]] = []  # TODO: ì‹¤ì œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ\n","\n","EMBEDDING_MODEL = \"BAAI/bge-m3\"\n","RERANKER_MODEL = \"BAAI/bge-reranker-v2-m3\"\n","emb_model_doc = SentenceTransformer(EMBEDDING_MODEL)\n","\n","def simple_tokenize_ko(text: str) -> List[str]:\n","    text = re.sub(r\"[^0-9A-Za-zê°€-í£%Â·\\.\\-\\s]\", \" \", text)\n","    return [t for t in text.split() if t]\n","\n","bm25_corpus_tokens = [simple_tokenize_ko(d[\"text\"]) for d in docs]\n","bm25 = BM25Okapi(bm25_corpus_tokens) if docs else None\n","\n","try:\n","    import faiss\n","    use_faiss = True\n","except Exception:\n","    faiss = None\n","    use_faiss = False\n","\n","if docs:\n","    doc_embeddings = emb_model_doc.encode([d[\"text\"] for d in docs], batch_size=64, convert_to_numpy=True, show_progress_bar=True)\n","    if use_faiss:\n","        dim = doc_embeddings.shape[1]\n","        index = faiss.IndexFlatIP(dim)\n","        norms = np.linalg.norm(doc_embeddings, axis=1, keepdims=True) + 1e-12\n","        normed = doc_embeddings / norms\n","        index.add(normed.astype('float32'))\n","    else:\n","        index = None\n","else:\n","    doc_embeddings = np.zeros((0, 384), dtype='float32')\n","    index = None\n","\n","def dense_search(query: str, top_k=40) -> List[Tuple[int, float]]:\n","    if not docs:\n","        return []\n","    q = emb_model_doc.encode([query], convert_to_numpy=True)[0]\n","    q = q / (np.linalg.norm(q) + 1e-12)\n","    if index is not None and use_faiss:\n","        D, I = index.search(q[np.newaxis, :].astype('float32'), top_k)\n","        return [(int(i), float(d)) for i, d in zip(I[0], D[0])]\n","    sims = (doc_embeddings @ q) / (np.linalg.norm(doc_embeddings, axis=1) + 1e-12)\n","    top_idx = np.argsort(-sims)[:top_k]\n","    return [(int(i), float(sims[i])) for i in top_idx]\n","\n","def sparse_search(query: str, top_k=80) -> List[Tuple[int, float]]:\n","    if not docs or bm25 is None:\n","        return []\n","    tokens = simple_tokenize_ko(query)\n","    scores = bm25.get_scores(tokens)\n","    top_idx = np.argsort(-scores)[:top_k]\n","    return [(int(i), float(scores[i])) for i in top_idx]\n","\n","def rrf_fuse(dense: List[Tuple[int, float]], sparse: List[Tuple[int, float]], k: int = 60, top_k: int = 50) -> List[int]:\n","    ranks: Dict[int, float] = {}\n","    for lst in [dense, sparse]:\n","        for rank, (idx, _) in enumerate(lst):\n","            ranks[idx] = ranks.get(idx, 0.0) + 1.0 / (k + rank + 1)\n","    fused = sorted(ranks.items(), key=lambda x: -x[1])[:top_k]\n","    return [idx for idx, _ in fused]\n","\n","try:\n","    reranker = CrossEncoder(RERANKER_MODEL)\n","except Exception:\n","    reranker = None\n","\n","def rerank(query: str, candidate_ids: List[int], top_k=10) -> List[int]:\n","    if not candidate_ids:\n","        return []\n","    pairs = [[query, docs[i][\"text\"]] for i in candidate_ids]\n","    if reranker:\n","        scores = reranker.predict(pairs)\n","    else:\n","        qset = set(simple_tokenize_ko(query))\n","        scores = [len(qset & set(simple_tokenize_ko(docs[i][\"text\"]))) for i in candidate_ids]\n","    order = np.argsort(-np.array(scores))[:top_k]\n","    return [candidate_ids[i] for i in order]\n","\n","def build_context(query: str, max_chars: int = 2000, top_k_dense=40, top_k_sparse=40, top_k_final=8) -> Tuple[str, List[Dict[str, Any]]]:\n","    dense = dense_search(query, top_k=top_k_dense)\n","    sparse = sparse_search(query, top_k=top_k_sparse)\n","    fused_ids = rrf_fuse(dense, sparse, k=60, top_k=50)\n","    final_ids = rerank(query, fused_ids, top_k=top_k_final)\n","\n","    selected = []\n","    total = 0\n","    for i in final_ids:\n","        d = docs[i]\n","        snippet = d[\"text\"][:800]\n","        selected.append({\"id\": d[\"id\"], \"title\": d.get(\"title\", \"\"), \"page\": d.get(\"page\", -1), \"text\": snippet})\n","        total += len(snippet)\n","        if total >= max_chars:\n","            break\n","\n","    ctx_blocks = []\n","    for s in selected:\n","        header = f\"[ë¬¸ì„œ:{s['id']}] ì œëª©:{s['title']} | í˜ì´ì§€:{s['page']}\"\n","        ctx_blocks.append(header + \"\\n\" + s[\"text\"].strip())\n","    context_text = \"\\n\\n\".join(ctx_blocks)\n","    return context_text, selected\n","\n","# ==================================================\n","# 3. LLM í”„ë¡¬í”„íŠ¸ & íŒŒì„œ\n","# ==================================================\n","SCHEMA_JSON = {\n","  \"type\": \"object\",\n","  \"properties\": {\n","    \"ì‚¬ìš©ì ì…ë ¥\": {\"type\": \"string\"},\n","    \"ì¶”ì²œ ì¹´í…Œê³ ë¦¬\": {\"type\": \"string\"},\n","    \"ì¶”ì²œ ì œí’ˆ\": {\"type\": \"string\"},\n","    \"ì „ë¬¸ê°€ì˜ ì˜ê²¬\": {\"type\": \"string\"},\n","    \"ê·¼ê±°\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n","    \"ì°¸ê³  ë¬¸ì„œ\": {\"type\": \"array\"}\n","  },\n","  \"required\": [\"ì‚¬ìš©ì ì…ë ¥\", \"ì¶”ì²œ ì œí’ˆ\", \"ì „ë¬¸ê°€ì˜ ì˜ê²¬\"]\n","}\n","\n","FEWSHOT = \"\"\"\n","{\n","  \"ì‚¬ìš©ì ì…ë ¥\": \"20ëŒ€ì—ê²Œ ì¶”ì²œí•˜ëŠ” ë¹„íƒ€ë¯¼\",\n","  \"ì¶”ì²œ ì¹´í…Œê³ ë¦¬\": \"ë¹„íƒ€ë¯¼\",\n","  \"ì¶”ì²œ ì œí’ˆ\": \"ì¢…í•© ë¹„íƒ€ë¯¼\",\n","  \"ì „ë¬¸ê°€ì˜ ì˜ê²¬\": \"20ëŒ€ëŠ” ê· í˜• ì¡íŒ ì˜ì–‘ ì„­ì·¨ê°€ í•„ìš”í•˜ë¯€ë¡œ ì¢…í•© ë¹„íƒ€ë¯¼ì´ ì í•©í•©ë‹ˆë‹¤.\",\n","  \"ê·¼ê±°\": [\"ë¦¬ë·° ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼\", \"ì œê³µ ë¬¸ì„œ ì°¸ì¡°\"]\n","}\n","\n","</ì˜ˆì‹œ>\n","ìœ„ëŠ” ì°¸ê³ ìš©ì…ë‹ˆë‹¤. ì¶œë ¥ ì‹œ ì˜ˆì‹œ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ê³ , ì‹¤ì œ ì‚¬ìš©ì ì…ë ¥ì— ë§ëŠ” ë‹µë³€ë§Œ ì‘ì„±í•˜ì„¸ìš”.\n","\n","ì—„ê²©í•œ ê·œì¹™:\n","- ë°˜ë“œì‹œ ìœ„ì˜ JSON í‚¤ë§Œ ì‚¬ìš©. ì¶”ê°€ í‚¤/ì£¼ì„/ë¨¸ë¦¬ë§ ê¸ˆì§€.\n","- \"ì „ë¬¸ê°€ì˜ ì˜ê²¬\"ì€ ì œê³µëœ ë¬¸ë§¥ì—ì„œë§Œ ì¸ìš©/ìš”ì•½. ëª¨ë¥´ë©´ \"ì •ë³´ ë¶€ì¡±\"ì´ë¼ê³  ëª…ì‹œ.\n","- ì œê³µëœ ë¬¸ë§¥ ë°– ì •ë³´ë¥¼ ì¶”ì •í•˜ê±°ë‚˜ ë°œëª… ê¸ˆì§€.\n","\"\"\"\n","\n","SYSTEM_INSTRUCTIONS = f\"\"\"\n","ë‹¹ì‹ ì€ ì˜ì–‘í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n","ì£¼ì–´ì§„ ë¦¬ë·° ê¸°ë°˜ ì¶”ì²œ + ë¬¸ì„œ ê¸°ë°˜ ê²€ìƒ‰ì„ í™œìš©í•˜ì—¬ JSONì„ ìƒì„±í•˜ì„¸ìš”.\n","ì ˆëŒ€ ë¬¸ë§¥ ë°– ì§€ì‹ìœ¼ë¡œ í™•ì¥í•˜ì§€ ë§ˆì„¸ìš”. ë¶ˆì¶©ë¶„í•˜ë©´ \"ì •ë³´ ë¶€ì¡±\"ì„ ì‚¬ìš©í•˜ì„¸ìš”\n","JSON ìŠ¤í‚¤ë§ˆ:\n","{json.dumps(SCHEMA_JSON, ensure_ascii=False)}\n","\"\"\"\n","\n","MODEL_NAME = os.environ.get(\"HF_LLM\", \"skt/A.X-4.0-Light\")\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n","    device_map=\"auto\"\n",")\n","textgen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n","\n","def robust_parse_json(text: str) -> Dict[str, Any]:\n","    m = re.search(r\"\\{[\\s\\S]*\\}\", text)\n","    if m:\n","        cand = m.group(0)\n","        try:\n","            return json.loads(cand)\n","        except Exception:\n","            pass\n","    return {\"ì‚¬ìš©ì ì…ë ¥\": \"\", \"ì¶”ì²œ ì œí’ˆ\": \"\", \"ì „ë¬¸ê°€ì˜ ì˜ê²¬\": \"\", \"ê·¼ê±°\": []}\n","\n","# ==================================================\n","# 4. ìµœì¢… ì§ˆì˜ ì²˜ë¦¬\n","# ==================================================\n","def answer_query(query: str, category: str, max_new_tokens: int = 256, temperature: float = 0.2) -> Dict[str, Any]:\n","    # ë¦¬ë·° ê¸°ë°˜ ì¶”ì²œ\n","    review_results = search_product_from_reviews(query, df)\n","    products = product_result(review_results, df)\n","\n","    # ë¬¸ì„œ ê¸°ë°˜ ê²€ìƒ‰\n","    context_text, selected = build_context(query)\n","\n","    prompt = f\"\"\"\n","    [ì‹œìŠ¤í…œ]\n","    {SYSTEM_INSTRUCTIONS}\n","\n","    [ë¬¸ë§¥]\n","    {context_text}\n","\n","    [ë¦¬ë·° ê¸°ë°˜ ì¶”ì²œ]\n","    {products}\n","\n","    [ì‚¬ìš©ì ì…ë ¥]\n","    {query}\n","\n","    [ì¹´í…Œê³ ë¦¬]\n","    {category}\n","\n","    [ì§€ì‹œ]\n","    {FEWSHOT}\n","    \"\"\"\n","\n","    out = textgen(\n","        prompt,\n","        max_new_tokens=max_new_tokens,\n","        do_sample=False,\n","        temperature=temperature,\n","        repetition_penalty=1.1,\n","        eos_token_id=tokenizer.eos_token_id,\n","        pad_token_id=tokenizer.eos_token_id,\n","        return_full_text=False\n","    )[0][\"generated_text\"]\n","\n","    data = robust_parse_json(out)\n","    if not data.get(\"ì¶”ì²œ ì¹´í…Œê³ ë¦¬\"):\n","        data[\"ì¶”ì²œ ì¹´í…Œê³ ë¦¬\"] = category\n","    if not data.get(\"ì°¸ê³  ë¬¸ì„œ\") and selected:\n","        data[\"ì°¸ê³  ë¬¸ì„œ\"] = [{\"title\": s[\"title\"], \"page\": s[\"page\"], \"ë¬¸ì„œID\": s[\"id\"]} for s in selected[:3]]\n","    return data\n","\n","# ==================================================\n","# 5. ì¶œë ¥ ë Œë”ë§\n","# ==================================================\n","def render_ko(data: Dict[str, Any]) -> str:\n","    parts = [\n","        f\"ì‚¬ìš©ì ì…ë ¥ : {data.get('ì‚¬ìš©ì ì…ë ¥','')}\",\n","        f\"ì¶”ì²œ ì¹´í…Œê³ ë¦¬ : {data.get('ì¶”ì²œ ì¹´í…Œê³ ë¦¬','')}\",\n","        f\"ì¶”ì²œ ì œí’ˆ : {data.get('ì¶”ì²œ ì œí’ˆ','')}\",\n","        f\"ì „ë¬¸ê°€ì˜ ì˜ê²¬ : {data.get('ì „ë¬¸ê°€ì˜ ì˜ê²¬','')}\"\n","    ]\n","    if data.get(\"ê·¼ê±°\"):\n","        parts.append(\"ê·¼ê±° : \" + \"; \".join(map(str, data[\"ê·¼ê±°\"])))\n","    return \"\\n\".join(parts)\n","\n","# ==================================================\n","# 6. ì‹¤í–‰ë¶€\n","# ==================================================\n","if __name__ == \"__main__\":\n","    query = input(\"ì‚¬ìš©ì ì§ˆë¬¸: \")\n","    result = answer_query(query, category)\n","    print(render_ko(result))\n"],"metadata":{"id":"dHj6AkMvDV_9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# RAG êµ¬í˜„(LlamaIndex ê¸°ë°˜)"],"metadata":{"id":"-lk0jf8bDd2n"}},{"cell_type":"code","source":["### í•„ìš”í•œ í•¨ìˆ˜ ì„í´íŠ¸\n","from llama_index.core.node_parser import SentenceSplitter\n","from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","import os\n","from dotenv import load_dotenv\n","from llama_index.llms.google_genai import GoogleGenAI\n","from llama_index.core.llms import ChatMessage, MessageRole\n","from llama_index.core import Settings\n","from llama_index.core import VectorStoreIndex, Document\n","from llama_index.core import SimpleDirectoryReader\n","import unicodedata\n","from llama_index.core.agent import ReActAgent\n","from llama_index.core.tools import QueryEngineTool, ToolMetadata\n","import json\n","\n","### SentenceSplitter\n","\n","text_splitter = SentenceSplitter(\n","    chunk_size=200,\n","    chunk_overlap=50\n",")\n","\n","### embedding\n","\n","# ì„ë² ë”© ëª¨ë¸ ìƒì„±\n","embed_model = HuggingFaceEmbedding(\n","    model_name = 'paraphrase-multilingual-MiniLM-L12-v2',\n","    device='cpu' # gpuê°€ ìˆë‹¤ë©´ 'cuda'\n",")\n","\n","### llm\n","\n","# google api key ë¶ˆëŸ¬ì˜¤ê¸°\n","google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n","google_api_key = ''\n","\n","\n","# llm ìƒì„±\n","llm = GoogleGenAI(\n","    model='gemini-2.5-pro',\n","    request_timeout=120.0,\n","    temperature=0.0,\n","    api_key=google_api_key\n",")\n","\n","### settings\n","\n","# llm, embedding, text_splitter ëª¨ë¸ ì„¤ì •\n","Settings.llm = llm\n","Settings.embed_model = embed_model\n","Settings.text_splitter = text_splitter\n","\n","### index\n","\n","import os\n","import unicodedata\n","\n","# ì§ì ‘ ë¶ˆëŸ¬ì˜¬ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n","file_path = \"/content/drive/MyDrive/á„‹á…µá†·á„‡á…¦á„ƒá…µá†¼/GCP gemini API á„’á…ªá†¯á„‹á…­á†¼/ì„±ë¶„ á„’á…­á„‚á…³á†¼.txt\"\n","\n","\n","# íŒŒì¼ ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n","if not os.path.exists(file_path):\n","    raise ValueError(f\"ì§€ì •ëœ íŒŒì¼ '{file_path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","\n","# íŒŒì¼ ê²½ë¡œë¥¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ì¤ë‹ˆë‹¤.\n","files = [file_path]\n","\n","# ì´ì œ 'files' ë¦¬ìŠ¤íŠ¸ì—ëŠ” ì§€ì •ëœ txt íŒŒì¼ì˜ ê²½ë¡œê°€ í•˜ë‚˜ë§Œ ë“¤ì–´ ìˆìŠµë‹ˆë‹¤.\n","# ì´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  RAGë¥¼ êµ¬ì¶•í•˜ë©´ ë©ë‹ˆë‹¤.\n","print(f\"ë¡œë“œí•  íŒŒì¼: {files}\")\n","\n","# ë¬¸ì„œ ë¡œë”©\n","txt_documents = SimpleDirectoryReader(input_files=files).load_data()\n","\n","# document ìë£Œ êµ¬ì¡° --> ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±: {'Node':ì„ë² ë”© ë²¡í„°,...}\n","'''\n","# VectorStoreIndex.from_documents(documents) ì‹¤í–‰\n","# ë‚´ë¶€ì ìœ¼ë¡œ ë¬¸ì„œ ë¶„í•  -> ì„ë² ë”© -> ì €ì¥ì´ ì‹¤í–‰\n","'''\n","\n","txt_index = VectorStoreIndex.from_documents(txt_documents)\n","\n","### query_engine\n","\n","# query_engine ìƒì„±\n","txt_engine = txt_index.as_query_engine(similarity_top_k=10, include_metadata=True)\n","\n","### queryenginetool\n","\n","## ë„êµ¬ ìƒì„±\n","\n","# pdf_tool ìƒì„±\n","txt_tool = QueryEngineTool(\n","        query_engine=txt_engine,\n","        metadata=ToolMetadata(\n","            name='txt_tool',\n","            description='ê±´ê°•ê¸°ëŠ¥ì‹í’ˆì— ë“¤ì–´ê°„ ì›ë£Œì˜ íš¨ëŠ¥ì„ ì•Œë ¤ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.'\n","        )\n","    )\n","\n","### agent\n","\n","# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜\n","react_system_prompt = f\"\"\"\n","ë‹¹ì‹ ì€ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ AI ë¹„ì„œì…ë‹ˆë‹¤. txt_toolì„ ë°˜ë“œì‹œ í™œìš©í•˜ì„¸ìš”.\n","\"\"\"\n","\n","# ReActAgent ìƒì„±\n","healthcare_agent = ReActAgent(\n","    tools=[txt_tool],\n","    llm=Settings.llm,\n","    system_prompt=react_system_prompt,\n","    verbose=True\n",")\n","\n","\n","response = await healthcare_agent.run(user_msg=f\"\"\"ë‹¹ì‹ ì€ ì†Œë¹„ìì—ê²Œ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ êµ¬ë§¤ ì‹œ ê¼­ ì•Œì•„ì•¼ í•  ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì „ë‹¬í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n","ì•„ë˜ì— ë‘ê°€ì§€ ì œí’ˆì´ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. {product_result}ì— ìˆëŠ” 'ìƒì„¸ì •ë³´'ì™€ txt_toolì— ìˆëŠ” ì›ë£Œì˜ íš¨ëŠ¥ í…ìŠ¤íŠ¸ë¥¼ ì´ìš©í•˜ì—¬ ë‘ê°€ì§€ ì œí’ˆì— ëŒ€í•œ íš¨ëŠ¥ì„ ìš”ì•½í•˜ì„¸ìš”.\n","íš¨ëŠ¥ì˜ ê·¼ê±°ë¥¼ ì°¾ê¸° ìœ„í•´ txt_toolì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ txt ë¬¸ì„œë¥¼ ì°¸ì¡°í•´ì•¼ í•©ë‹ˆë‹¤.\n","ì œí’ˆ ì •ë³´ì—ëŠ” ê³ ìœ ë²ˆí˜¸ëŠ” ì ì§€ ë§ˆì„¸ìš”.\n","ë¶€ì • ë¦¬ë·°ì™€ ì¤‘ë¦½ ë¦¬ë·° ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì œí’ˆì„ êµ¬ë§¤í•˜ê¸° ì „ì— ì‚¬ìš©ìê°€ 'ì£¼ì˜í•´ì•¼ í•  ì 'ì„ ì„¸ ê°€ì§€ë¡œ ìš”ì•½í•˜ì„¸ìš”.\n","ê° ì£¼ì˜í•  ì ì€ ì§§ì€ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•˜ê³ , '~ê°€ ìˆìœ¼ë‹ˆ ì£¼ì˜í•˜ì„¸ìš”.'ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n","ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°ì—ëŠ” 'ì¶œì²˜ ì—†ìŒ'ì´ë¼ê³  ì‘ì„±í•˜ì„¸ìš”.\n","\n","ë‘ê°€ì§€ ì œí’ˆ : {product_result}\n","\n","ì¶œë ¥ì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤:\n","[\n","    {{\n","      \"ì œí’ˆëª…\": \"...\",\n","      \"ì œí’ˆì˜ íš¨ëŠ¥\": \"...\",\n","      \"ì œí’ˆ ì •ë³´\": \"...\",\n","      \"ê·¼ê±°\": \"...\" - txt_toolì„ ì°¸ì¡°í•˜ì„¸ìš”,\n","      \"ê¸ì • ë¦¬ë·°\": \"...\",\n","      \"ì£¼ì˜í•  ì \": \"...\",\n","      \"ìš”ì•½\": \"...\"\n","    }},\n","    ...\n","  ]\"\"\")\n","print(str(response))"],"metadata":{"id":"BLjWVyRxDgEI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ì›¹ í˜ì´ì§€ êµ¬í˜„"],"metadata":{"id":"lLtNlexhDmuu"}},{"cell_type":"code","source":["import streamlit as st\n","import pandas as pd\n","import numpy as np\n","import ast\n","import json\n","import os\n","import re\n","from sklearn.metrics.pairwise import cosine_similarity\n","from llama_index.core.node_parser import SentenceSplitter\n","from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","from llama_index.llms.google_genai import GoogleGenAI\n","from llama_index.core import Settings\n","from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n","from dotenv import load_dotenv\n","from typing import Any, Dict, List, Optional, Union\n","\n","# .env íŒŒì¼ ë¡œë“œ (ê°™ì€ í´ë”ì— ë³¸ì¸ì˜ êµ¬ê¸€ api ì£¼ì†Œë¥¼ ë„£ì€ .env íŒŒì¼ ìƒì„± í™•ì¸!)\n","load_dotenv()\n","\n","# --- 1. ì´ˆê¸° ì„¤ì • ë° ìì› ë¡œë”© í•¨ìˆ˜ ---\n","def safe_parse_embedding(x):\n","    \"\"\"ê²°ì¸¡ì¹˜(NaN)ë‚˜ ë¹„ì •ìƒ ë¬¸ìì—´ì„ ì•ˆì „í•˜ê²Œ íŒŒì‹±í•©ë‹ˆë‹¤.\"\"\"\n","    try:\n","        return ast.literal_eval(x) if pd.notna(x) else []\n","    except Exception:\n","        return []\n","\n","@st.cache_data(show_spinner=False)\n","def load_data():\n","    \"\"\"CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì„ë² ë”© ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"\"\"\n","    file_path = 'reviews_chunked_embeddings4.csv' # íŒŒì¼ë„ ê°™ì€ í´ë”ì— ìˆì–´ì•¼ í•¨!\n","    if not os.path.exists(file_path):\n","        st.error(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n","        st.stop()\n","    df = pd.read_csv(file_path)\n","    df['ê¸ì •ë¦¬ë·°_ì„ë² ë”©_int'] = df['ê¸ì •ë¦¬ë·°_ì„ë² ë”©'].apply(safe_parse_embedding)\n","    return df\n","\n","@st.cache_resource(show_spinner=False)\n","def load_all_resources():\n","    \"\"\"RAG ì¸ë±ìŠ¤ì™€ í•„ìš”í•œ ëª¨ë“  ìì›ì„ í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n","    with st.spinner(\"ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...\"):\n","        # ì„ë² ë”© ëª¨ë¸\n","        embed_model = HuggingFaceEmbedding(\n","            model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n","            device='cpu' # GPUê°€ ìˆë‹¤ë©´ 'cuda'\n","        )\n","\n","        # LLM\n","        google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n","        if not google_api_key:\n","            st.error(\"GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n","            st.stop()\n","        llm = GoogleGenAI(\n","            model='gemini-2.5-pro',\n","            request_timeout=120.0,\n","            temperature=0.0,\n","            api_key=google_api_key\n","        )\n","\n","        # ì„¤ì • ë“±ë¡\n","        Settings.llm = llm\n","        Settings.embed_model = embed_model\n","        Settings.text_splitter = SentenceSplitter(chunk_size=200, chunk_overlap=50)\n","\n","        # RAG ë°ì´í„° ë¡œë“œ ë° ì¸ë±ì‹±\n","        txt_path = \"ì„±ë¶„ íš¨ëŠ¥.txt\" # í•´ë‹¹ í…ìŠ¤íŠ¸ íŒŒì¼ë„ ê°™ì€ í´ë” ìƒì— ìˆëŠ”ì§€ í™•ì¸ ë°”ëŒ!\n","        if not os.path.exists(txt_path):\n","            st.error(f\"RAGì— í•„ìš”í•œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {txt_path}\")\n","            st.stop()\n","        txt_documents = SimpleDirectoryReader(input_files=[txt_path]).load_data()\n","        txt_index = VectorStoreIndex.from_documents(txt_documents)\n","        txt_engine = txt_index.as_query_engine(similarity_top_k=10, include_metadata=True)\n","\n","    return {\n","        \"df_reviews\": load_data(),\n","        \"llm\": llm,\n","        \"embed_model\": embed_model,\n","        \"txt_engine\": txt_engine\n","    }\n","\n","# --- Helper functions ---\n","def extract_json_from_text(s: str) -> Optional[str]:\n","    \"\"\"í…ìŠ¤íŠ¸ì—ì„œ JSON ë¬¸ìì—´ì„ ë‹¤ë‹¨ê³„ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n","    # 1) ```json``` ë¸”ë¡ ì¶”ì¶œ\n","    m = re.search(r'```json\\s*(\\[.*\\])\\s*```', s, re.DOTALL)\n","    if m:\n","        return m.group(1)\n","\n","    # 2) ì „ì²´ì—ì„œ ì²« '[' ~ ë§ˆì§€ë§‰ ']'ê¹Œì§€ ì¶”ì¶œ\n","    first = s.find('[')\n","    last = s.rfind(']')\n","    if first != -1 and last != -1 and last > first:\n","        return s[first:last+1]\n","\n","    # 3) ë¼ì¸ ë‹¨ìœ„ë¡œ ì¡°í•© ì‹œë„ (ë°°ì—´ ì‹œì‘/ë ë˜ëŠ” ê°ì²´ ì‹œì‘)\n","    lines = s.splitlines()\n","    json_lines = [line for line in lines if line.strip().startswith('[') or line.strip().endswith(']') or line.strip().startswith('{') or (line.strip().startswith('\"') and line.strip().endswith(',')) or (line.strip().startswith('}') and line.strip().endswith(','))]\n","\n","    if json_lines:\n","        combined_json = \"\\n\".join(json_lines)\n","        if combined_json.strip():\n","            return combined_json.strip()\n","\n","    return None\n","\n","# --- 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë° RAG ë¡œì§ í•¨ìˆ˜ ---\n","def calculate_recommendations(\n","    filtered_df: pd.DataFrame,\n","    user_query: str,\n","    selected_keywords: List[str],\n","    embed_model: HuggingFaceEmbedding,\n","    top_n: int = 2\n",") -> pd.DataFrame:\n","    \"\"\"\n","    ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ RAG ë¡œì§ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì¿¼ë¦¬ì— ê°€ì¥ ì í•©í•œ\n","    ì œí’ˆì„ ì¶”ì²œí•˜ëŠ” í•¨ìˆ˜.\n","\n","    Args:\n","        filtered_df (pd.DataFrame): 'ê¸ì •ë¦¬ë·°_ì„ë² ë”©_int' ì»¬ëŸ¼ì„ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.\n","        user_query (str): ì‚¬ìš©ìì˜ ê²€ìƒ‰ ì¿¼ë¦¬.\n","        selected_keywords (List[str]): ì‚¬ìš©ìê°€ ì„ íƒí•œ ì¹´í…Œê³ ë¦¬(í‚¤ì›Œë“œ) ë¦¬ìŠ¤íŠ¸.\n","        embed_model (HuggingFaceEmbedding): ë¯¸ë¦¬ ë¡œë“œëœ ì„ë² ë”© ëª¨ë¸ ê°ì²´.\n","        top_n (int): ì¶”ì²œí•  ì œí’ˆì˜ ê°œìˆ˜.\n","\n","    Returns:\n","        pd.DataFrame: ì¶”ì²œ ì œí’ˆ ì •ë³´ê°€ ë‹´ê¸´ DataFrame.\n","    \"\"\"\n","    # 1. ì¸ë±ìŠ¤ ì¬ì„¤ì • ë° ë°ì´í„° ë³µì‚¬ (ì¸ë±ìŠ¤ ì—ëŸ¬ í•´ê²°)\n","    df = filtered_df.reset_index(drop=True)\n","\n","    if df.empty:\n","        return pd.DataFrame()\n","\n","    # 2. ì‚¬ìš©ì ì¿¼ë¦¬ ì„ë² ë”©\n","    combined_query = \" \".join(selected_keywords) + \" \" + user_query\n","    query_embedding = embed_model.get_text_embedding(combined_query.strip())\n","\n","    # 3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n","    embeddings_matrix = np.array(df['ê¸ì •ë¦¬ë·°_ì„ë² ë”©_int'].tolist())\n","    similarities = cosine_similarity([query_embedding], embeddings_matrix)[0]\n","\n","    # 4. ê° ì œí’ˆì˜ ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì°¾ì•„ì„œ DataFrameì— ì¶”ê°€\n","    df['ìœ ì‚¬ë„'] = similarities\n","\n","    # 5. ì œí’ˆë³„ë¡œ ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ í–‰ì„ ì„ íƒ\n","    idx = df.groupby('ê³ ìœ ë²ˆí˜¸')['ìœ ì‚¬ë„'].idxmax()\n","    df_unique = df.loc[idx].sort_values(by='ìœ ì‚¬ë„', ascending=False)\n","\n","    # 6. ìƒìœ„ nê°œ ì œí’ˆë§Œ ì„ íƒ\n","    recommended_df = df_unique.head(top_n).copy()\n","\n","    # 7. ìœ ì‚¬ë„ ì ìˆ˜ê°€ 0ì¸ ê²½ìš° ì¶”ì²œ ì œí’ˆ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬\n","    if recommended_df.empty or recommended_df['ìœ ì‚¬ë„'].iloc[0] == 0.0:\n","        return pd.DataFrame()\n","\n","    return recommended_df\n","\n","def answer_query(txt_engine: Any, products_info_json: str) -> Optional[List[Dict[str, Any]]]:\n","    \"\"\"\n","    QueryEngineì„ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ ì „ë¬¸ê°€ ì˜ê²¬ì„ ìƒì„±í•©ë‹ˆë‹¤.\n","    \"\"\"\n","    user_prompt = f\"\"\"\n","    ë‹¹ì‹ ì€ ì†Œë¹„ìì—ê²Œ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ êµ¬ë§¤ ì‹œ ê¼­ ì•Œì•„ì•¼ í•  ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì „ë‹¬í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n","    **ë°˜ë“œì‹œ ë‹¹ì‹ ì—ê²Œ ì œê³µëœ ë¬¸ì„œ(ì„±ë¶„ íš¨ëŠ¥.txt)ì— ìˆëŠ” ì •ë³´ë§Œì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì •ë³´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**\n","\n","    ì•„ë˜ì— ë‘ê°€ì§€ ì œí’ˆì´ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. {products_info_json}ì— ìˆëŠ” 'ìƒì„¸ì •ë³´'ì™€ **ì œê³µëœ ë¬¸ì„œ(ì„±ë¶„ íš¨ëŠ¥.txt)ì— ìˆëŠ” ì›ë£Œì˜ íš¨ëŠ¥ í…ìŠ¤íŠ¸ë¥¼ ì´ìš©í•˜ì—¬** ë‘ê°€ì§€ ì œí’ˆì— ëŒ€í•œ íš¨ëŠ¥ì„ ìš”ì•½í•˜ì„¸ìš”.\n","    íš¨ëŠ¥ì˜ ê·¼ê±°ë¥¼ ì°¾ê¸° ìœ„í•´ ì œê³µëœ ë¬¸ì„œë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n","    ì œí’ˆ ì •ë³´ì—ëŠ” ê³ ìœ ë²ˆí˜¸ëŠ” ì ì§€ ë§ˆì„¸ìš”.\n","\n","    **ê° ì œí’ˆì— ëŒ€í•œ ê¸ì •, ë¶€ì •, ì¤‘ë¦½ ë¦¬ë·°ì˜ ì‹¤ì œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê°ê°ì„ ìš”ì•½í•´ì•¼ í•©ë‹ˆë‹¤.** ë§Œì•½ ë¦¬ë·° ë‚´ìš©ì´ ì—†ë‹¤ë©´ 'ì •ë³´ ì—†ìŒ'ì´ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.\n","\n","    ë¶€ì • ë¦¬ë·°ì™€ ì¤‘ë¦½ ë¦¬ë·° ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì œí’ˆì„ êµ¬ë§¤í•˜ê¸° ì „ì— ì‚¬ìš©ìê°€ 'ì£¼ì˜í•´ì•¼ í•  ì 'ì„ ì„¸ ê°€ì§€ë¡œ ìš”ì•½í•˜ì„¸ìš”.\n","    ê° ì£¼ì˜í•  ì ì€ ì§§ì€ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•˜ê³ , '~ê°€ ìˆìœ¼ë‹ˆ ì£¼ì˜í•˜ì„¸ìš”.'ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n","    ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°ì—ëŠ” 'ì¶œì²˜ ì—†ìŒ'ì´ë¼ê³  ì‘ì„±í•˜ì„¸ìš”.\n","\n","    ë‘ê°€ì§€ ì œí’ˆ : {products_info_json}\n","\n","    ì¶œë ¥ì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤:\n","    [\n","        {{\n","          \"ì œí’ˆëª…\": \"...\",\n","          \"ì œí’ˆì˜ íš¨ëŠ¥\": \"...\",\n","          \"ì œí’ˆ ì •ë³´\": \"...\",\n","          \"ê·¼ê±°\": \"...\" - ì œê³µëœ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”,\n","          \"ê¸ì • ë¦¬ë·°\": \"...\",\n","          \"ë¶€ì • ë¦¬ë·°\": \"...\",\n","          \"ì¤‘ë¦½ ë¦¬ë·°\": \"...\",\n","          \"ì£¼ì˜í•  ì \": \"...\",\n","          \"ìš”ì•½\": \"...\"\n","        }},\n","        ...\n","      ]\"\"\"\n","\n","    # 1. QueryEngine ì§ì ‘ í˜¸ì¶œ\n","    try:\n","        response = txt_engine.query(user_prompt)\n","        response_str = str(response)\n","    except Exception as e:\n","        st.error(f\"ì¿¼ë¦¬ ì—”ì§„ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n","        return None\n","\n","    # 2. JSON íŒŒì‹±\n","    json_str = extract_json_from_text(response_str)\n","\n","    if json_str:\n","        try:\n","            return json.loads(json_str)\n","        except Exception:\n","            pass\n","\n","    # ìµœì¢…ì ìœ¼ë¡œ ì „ì²´ ë¬¸ìì—´ì—ì„œ íŒŒì‹± ì‹œë„\n","    try:\n","        return json.loads(response_str)\n","    except (json.JSONDecodeError, SyntaxError) as e:\n","        st.error(\"LLM ì‘ë‹µì—ì„œ JSONì„ íŒŒì‹±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n","        st.json(response_str)\n","        return None\n","\n","# --- 3. Streamlit ì•± ë©”ì¸ UI ---\n","def main():\n","    if \"page\" not in st.session_state:\n","        st.session_state.page = 1\n","        st.session_state.df_reviews = pd.DataFrame()\n","        st.session_state.selected_supplement = None\n","        st.session_state.selected_supplement_name = None\n","        st.session_state.selected_keywords = []\n","        st.session_state.page2_checkbox_states = {}\n","        st.session_state.filtered_data = pd.DataFrame()\n","        st.session_state.user_free_text = \"\"\n","        st.session_state.recommended_products = pd.DataFrame()\n","        st.session_state.supplementary_reviews = pd.DataFrame()\n","        st.session_state.rag_result = None\n","\n","    # ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì‚¬ì „ ë¡œë“œ\n","    resources = load_all_resources()\n","    st.session_state.df_reviews = resources.get(\"df_reviews\")\n","    st.session_state.llm = resources.get(\"llm\")\n","    st.session_state.embed_model = resources.get(\"embed_model\")\n","    st.session_state.txt_engine = resources.get(\"txt_engine\")\n","\n","    # --- í˜ì´ì§€ë³„ UI êµ¬ì„± ---\n","    # Page 1\n","    if st.session_state.page == 1:\n","        st.markdown(\"<h1 style='text-align: center;'>ğŸ’Š ê±´ê°•ì‹í’ˆ ì¶”ì²œ ì±—ë´‡</h1>\", unsafe_allow_html=True)\n","        st.markdown(\"<h5 style='text-align: center;'>ë¦¬ë·°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ì¶¤í˜• ê±´ê°•ì‹í’ˆì„ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤.</h5>\", unsafe_allow_html=True)\n","        st.progress(25)\n","        st.markdown(\"ë‹¨ê³„ **1/4** | 25% ì™„ë£Œ\")\n","        st.divider()\n","        st.markdown(\"<h3 style='text-align: center;'>ì–´ë–¤ ê±´ê°•ì‹í’ˆì„ ì°¾ìœ¼ì‹œë‚˜ìš”?</h3>\", unsafe_allow_html=True)\n","\n","        supplements = {\n","            \"ë‹¨ë°±ì§ˆ ì‰ì´í¬\": {\"description\": \"ğŸ’ª ê·¼ìœ¡ ê±´ê°• ë° ìš´ë™ í›„ íšŒë³µ\", \"csv_name\": \"ë‹¨ë°±ì§ˆ\"},\n","            \"ì˜¤ë©”ê°€3\": {\"description\": \"ğŸŸ ì‹¬í˜ˆê´€ ê±´ê°• ë° ë‡Œ ê¸°ëŠ¥\", \"csv_name\": \"ì˜¤ë©”ê°€3\"},\n","            \"ì¢…í•©ë¹„íƒ€ë¯¼\": {\"description\": \"ğŸŒˆ ì „ë°˜ì ì¸ ì˜ì–‘ ê· í˜•\", \"csv_name\": \"ë¹„íƒ€ë¯¼\"},\n","            \"ì•„ì—°\": {\"description\": \"âš¡ï¸ ë©´ì—­ë ¥ ë° í”¼ë¶€ ê±´ê°•\", \"csv_name\": \"ì•„ì—°\"},\n","            \"ìœ ì‚°ê· \": {\"description\": \"ğŸŒ¿ ì¥ ê±´ê°• ë° ì†Œí™” ê¸°ëŠ¥\", \"csv_name\": \"ìœ ì‚°ê· \"}\n","        }\n","\n","        for supplement_name, data_item in supplements.items():\n","            if st.button(f\"**{supplement_name}**\\n\\n{data_item['description']}\", use_container_width=True, key=supplement_name):\n","                st.session_state.selected_supplement = data_item['csv_name']\n","                st.session_state.selected_supplement_name = supplement_name\n","                st.success(f\"**{supplement_name}**ë¥¼ ì„ íƒí•˜ì…¨ìŠµë‹ˆë‹¤.\")\n","                st.session_state.page = 2\n","                st.rerun()\n","\n","        st.divider()\n","        col1, col2 = st.columns([0.89, 0.11])\n","        with col1:\n","            st.button(\"ì´ì „\", disabled=True)\n","        with col2:\n","            if st.button(\"ë‹¤ìŒ >\", disabled=not st.session_state.selected_supplement):\n","                if st.session_state.selected_supplement:\n","                    st.session_state.page = 2\n","                    st.rerun()\n","                else:\n","                    st.warning(\"ë¨¼ì € ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.\")\n","\n","    # Page 2\n","    elif st.session_state.page == 2:\n","        df_reviews = st.session_state.df_reviews\n","        selected_category = st.session_state.get('selected_supplement', None)\n","        if selected_category and not df_reviews.empty:\n","            st.session_state.filtered_data = df_reviews[df_reviews['ì¹´í…Œê³ ë¦¬'] == selected_category].copy()\n","            if st.session_state.filtered_data.empty:\n","                st.warning(\"ì„ íƒí•˜ì‹  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.\")\n","                st.session_state.page = 1\n","                st.stop()\n","\n","        st.markdown(\"<h1 style='text-align: center;'>ğŸ’Š ê±´ê°•ì‹í’ˆ ì¶”ì²œ ì±—ë´‡</h1>\", unsafe_allow_html=True)\n","        st.markdown(\"<h5 style='text-align: center;'>ë¦¬ë·°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ì¶¤í˜• ê±´ê°•ì‹í’ˆì„ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤.</h5>\", unsafe_allow_html=True)\n","        st.progress(50)\n","        st.markdown(\"ë‹¨ê³„ **2/4** | 50% ì™„ë£Œ\")\n","        st.divider()\n","        st.markdown(\"<h3 style='text-align: center;'>ì£¼ìš” ê´€ì‹¬ì‚¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)</h3>\", unsafe_allow_html=True)\n","\n","        supplement_keywords = {\n","            \"ë‹¨ë°±ì§ˆ\": [\"ê·¼ìœ¡ ì¦ê°€\", \"ë§›ìˆëŠ”\", \"ëŒ€ìš©ëŸ‰\", \"ëª© ë„˜ê¹€ì´ í¸í•œ\"],\n","            \"ì˜¤ë©”ê°€3\": [\"ì‹¬í˜ˆê´€ ê±´ê°•\", \"ê´€ì ˆ ê±´ê°•\", \"ì•Œì•½ í¬ê¸°\", \"ìƒì„  ë§› ì•ˆë‚˜ëŠ”\"],\n","            \"ë¹„íƒ€ë¯¼\": [\"í”¼ë¡œ íšŒë³µ\", \"ë¸Œëœë“œ ì‹ ë¢°ì„±\", \"ì•Œì•½ í¬ê¸°\", \"ê³ í•¨ëŸ‰\"],\n","            \"ì•„ì—°\": [\"ë©´ì—­ë ¥ ê°•í™”\", \"ì•Œì•½ í¬ê¸°\", \"í•˜ë£¨ ì„­ì·¨ëŸ‰\", \"ë¶€ì‘ìš© ì—†ëŠ”\"],\n","            \"ìœ ì‚°ê· \": [\"ì¥ ê±´ê°•\", \"ì†Œí™” ê°œì„ \", \"í™œë ¥ ì¦ì§„\", \"ë°°ë³€ í™œë™\"],\n","        }\n","        selected_supplement = st.session_state.get('selected_supplement', None)\n","        keywords = supplement_keywords.get(selected_supplement, [])\n","        selected_keywords = []\n","\n","        cols = st.columns(2)\n","        for i, keyword in enumerate(keywords):\n","            col_index = i % 2\n","            with cols[col_index]:\n","                if f\"chkbox_{keyword}\" not in st.session_state.page2_checkbox_states:\n","                    st.session_state.page2_checkbox_states[f\"chkbox_{keyword}\"] = False\n","                is_checked = st.checkbox(\n","                    label=keyword,\n","                    key=f\"chkbox_{keyword}\",\n","                    value=st.session_state.page2_checkbox_states[f\"chkbox_{keyword}\"]\n","                )\n","                if is_checked:\n","                    selected_keywords.append(keyword)\n","\n","        st.session_state.selected_keywords = selected_keywords\n","        st.divider()\n","        col1, col2 = st.columns([0.89, 0.11])\n","        with col1:\n","            if st.button(\"ì´ì „\"):\n","                st.session_state.page = 1\n","                st.rerun()\n","        with col2:\n","            if st.button(\"ë‹¤ìŒ >\"):\n","                if st.session_state.selected_keywords:\n","                    st.session_state.page = 3\n","                    st.rerun()\n","                else:\n","                    st.warning(\"í•˜ë‚˜ ì´ìƒì˜ ê´€ì‹¬ì‚¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.\")\n","\n","    # Page 3\n","    elif st.session_state.page == 3:\n","        st.markdown(\"<h1 style='text-align: center;'>ğŸ’Š ê±´ê°•ì‹í’ˆ ì¶”ì²œ ì±—ë´‡</h1>\", unsafe_allow_html=True)\n","        st.markdown(\"<h5 style='text-align: center;'>ë¦¬ë·°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ì¶¤í˜• ê±´ê°•ì‹í’ˆì„ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤.</h5>\", unsafe_allow_html=True)\n","        st.progress(75)\n","        st.markdown(\"ë‹¨ê³„ **3/4** | 75% ì™„ë£Œ\")\n","        st.divider()\n","        st.markdown(\n","            \"\"\"\n","            <style>\n","            .stTextArea > label { display: none; }\n","            </style>\n","            \"\"\",\n","            unsafe_allow_html=True\n","        )\n","        st.markdown(\"<h3 style='text-align: center;'>ì¶”ê°€ ìš”ì²­ì‚¬í•­ì„ ììœ ë¡­ê²Œ ì…ë ¥í•´ì£¼ì„¸ìš”</h3>\", unsafe_allow_html=True)\n","        st.session_state.user_free_text = st.text_area(\n","            label=\"ì§ˆë¬¸ ì…ë ¥ì°½\",\n","            placeholder=\"ì˜ˆ: ì•Œë ˆë¥´ê¸°ê°€ ìˆì–´ì„œ íŠ¹ì • ì„±ë¶„ì€ í”¼í•˜ê³  ì‹¶ì–´ìš”,\",\n","            height=150,\n","            value=st.session_state.user_free_text,\n","            key=\"user_input_text\"\n","        )\n","        st.markdown(\"<small>ì„ íƒì‚¬í•­ì…ë‹ˆë‹¤. ì´ ë‹¨ê³„ëŠ” ê±´ë„ˆë›¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</small>\", unsafe_allow_html=True)\n","        st.divider()\n","        col1, col2 = st.columns([0.86, 0.14])\n","        with col1:\n","            if st.button(\"ì´ì „\"):\n","                st.session_state.page = 2\n","                st.rerun()\n","        with col2:\n","            if st.button(\"ì¶”ì²œ ë°›ê¸°\"):\n","                if st.session_state.filtered_data.empty:\n","                    st.warning(\"ì„ íƒí•˜ì‹  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ë¦¬ë·° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.\")\n","                    st.stop()\n","\n","                # ì‚¬ìš©ì ì¿¼ë¦¬ ì¡°í•©\n","                user_query = st.session_state.user_free_text.strip()\n","\n","                if not st.session_state.selected_keywords and not user_query:\n","                    st.warning(\"ê´€ì‹¬ì‚¬ ë˜ëŠ” ì¶”ê°€ ìš”ì²­ì‚¬í•­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n","                    st.stop()\n","\n","                with st.spinner(\"ìµœì ì˜ ì œí’ˆì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤...\"):\n","                    recommended_products = calculate_recommendations(\n","                        st.session_state.filtered_data,\n","                        user_query,\n","                        st.session_state.selected_keywords,\n","                        st.session_state.embed_model\n","                    )\n","                    st.session_state.recommended_products = recommended_products\n","\n","                # --- ì´ ë¶€ë¶„ì„ ì¶”ê°€/ìˆ˜ì •í•´ì£¼ì„¸ìš”! ---\n","                if recommended_products.empty:\n","                    st.warning(\"ì„ íƒí•˜ì‹  ì¡°ê±´ì— ë§ëŠ” ì¶”ì²œ ì œí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¡°ê±´ì„ ì™„í™”í•˜ê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.\")\n","                    st.session_state.recommended_products = recommended_products # ë¹ˆ DataFrame ì €ì¥\n","                    st.session_state.rag_result = None # RAG ê²°ê³¼ë„ ì´ˆê¸°í™”\n","                    st.session_state.page = 4\n","                    st.rerun()\n","\n","                with st.spinner(\"ì „ë¬¸ê°€ ì˜ê²¬ì„ ìƒì„± ì¤‘...\"):\n","                    products_info_list = []\n","                    for _, product_row in recommended_products.iterrows():\n","                        all_reviews = st.session_state.df_reviews[st.session_state.df_reviews['ì œí’ˆëª…'] == product_row['ì œí’ˆëª…']]\n","                        negative_reviews = ' '.join(all_reviews['ë¶€ì •ë¦¬ë·°'].dropna().astype(str).tolist())\n","                        neutral_reviews = ' '.join(all_reviews['ì¤‘ë¦½ë¦¬ë·°'].dropna().astype(str).tolist())\n","\n","                        products_info_list.append({\n","                            \"ì œí’ˆëª…\": product_row['ì œí’ˆëª…'],\n","                            \"ê³ ìœ ë²ˆí˜¸\": product_row['ê³ ìœ ë²ˆí˜¸'],\n","                            \"ìƒì„¸ì •ë³´\": product_row['ìƒì„¸ì •ë³´'],\n","                            \"ê¸ì •ë¦¬ë·°\": product_row['ê¸ì •ë¦¬ë·°'],\n","                            \"ë¶€ì •ë¦¬ë·°\": negative_reviews,\n","                            \"ì¤‘ë¦½ë¦¬ë·°\": neutral_reviews,\n","                            \"ì¹´í…Œê³ ë¦¬\": product_row['ì¹´í…Œê³ ë¦¬']\n","                        })\n","\n","                    rag_result = answer_query(\n","                        st.session_state.txt_engine,\n","                        json.dumps(products_info_list, ensure_ascii=False)\n","                    )\n","\n","                    st.session_state.rag_result = rag_result\n","                    st.session_state.page = 4\n","                    st.rerun()\n","\n","    # Page 4\n","    elif st.session_state.page == 4:\n","        st.markdown(\"<h1 style='text-align: center;'>ê±´ê°•ì‹í’ˆ ì¶”ì²œ ê²°ê³¼</h1>\", unsafe_allow_html=True)\n","        col1, col2 = st.columns([0.8, 0.2])\n","\n","        with col2:\n","            if st.button(\"ë‹¤ì‹œ ê²€ìƒ‰\", use_container_width=True):\n","                st.session_state.page = 1\n","                st.session_state.selected_supplement = None\n","                st.session_state.selected_supplement_name = None\n","                st.session_state.selected_keywords = []\n","                st.session_state.page2_checkbox_states = {}\n","                st.session_state.filtered_data = pd.DataFrame()\n","                st.session_state.user_free_text = \"\"\n","                st.session_state.recommended_products = pd.DataFrame()\n","                st.session_state.rag_result = None\n","                st.rerun()\n","\n","        st.progress(100)\n","        st.markdown(\"ë‹¨ê³„ **4/4** | 100% ì™„ë£Œ\")\n","        st.divider()\n","\n","        st.markdown(\"### ğŸ‘¨â€ğŸ’¼ ì‚¬ìš©ì ì •ë³´\")\n","        with st.container():\n","            st.markdown(\"<div style='border:1px solid #eee; padding:12px; border-radius:8px'>\", unsafe_allow_html=True)\n","            st.markdown(f\"**ê´€ì‹¬ ì œí’ˆ:** {st.session_state.get('selected_supplement_name', 'ì„ íƒë˜ì§€ ì•ŠìŒ')}\")\n","            st.markdown(f\"**ê´€ì‹¬ì‚¬:** {', '.join(st.session_state.get('selected_keywords', []))}\")\n","            if st.session_state.get('user_free_text'):\n","                st.markdown(f\"**ì¶”ê°€ ìš”ì²­ì‚¬í•­:** {st.session_state.user_free_text}\")\n","            st.markdown(\"</div>\", unsafe_allow_html=True)\n","\n","        st.markdown(\"### ğŸ§‘â€âš•ï¸ ì¶”ì²œ ì œí’ˆ ë° ì „ë¬¸ê°€ ì˜ê²¬\") # ì œëª© ë³€ê²½\n","        with st.container():\n","            st.markdown(\"<div style='border:1px solid #eee; padding:12px; border-radius:8px'>\", unsafe_allow_html=True)\n","            rag_result = st.session_state.get('rag_result', None)\n","            if rag_result and isinstance(rag_result, list):\n","                for product_data in rag_result:\n","                    if 'ì œí’ˆëª…' in product_data:\n","                        st.markdown(f\"#### {product_data.get('ì œí’ˆëª…', 'ì œí’ˆëª… ì—†ìŒ')}\")\n","                        st.markdown(f\"**ì œí’ˆì˜ íš¨ëŠ¥:** {product_data.get('ì œí’ˆì˜ íš¨ëŠ¥', 'ì •ë³´ ì—†ìŒ')}\")\n","                        st.markdown(f\"**ì œí’ˆ ì •ë³´:** {product_data.get('ì œí’ˆ ì •ë³´', 'ì •ë³´ ì—†ìŒ')}\")\n","                        st.markdown(f\"**ê·¼ê±°:** {product_data.get('ê·¼ê±°', 'ì¶œì²˜ ì—†ìŒ')}\")\n","                        st.markdown(f\"**ìš”ì•½:** {product_data.get('ìš”ì•½', 'ì •ë³´ ì—†ìŒ')}\")\n","                        st.markdown(\"---\")\n","                        st.markdown(\"##### ğŸ“ ì°¸ê³ í• ë§Œí•œ ë¦¬ë·°ë“¤\")\n","                        st.markdown(f\"- **ê¸ì • ë¦¬ë·°**: {product_data.get('ê¸ì • ë¦¬ë·°', 'ì •ë³´ ì—†ìŒ')}\")\n","                        st.markdown(f\"- **ë¶€ì • ë¦¬ë·°**: {product_data.get('ë¶€ì • ë¦¬ë·°', 'ì •ë³´ ì—†ìŒ')}\")\n","                        st.markdown(f\"- **ì¤‘ë¦½ ë¦¬ë·°**: {product_data.get('ì¤‘ë¦½ ë¦¬ë·°', 'ì •ë³´ ì—†ìŒ')}\")\n","                        st.markdown(f\"**ì£¼ì˜í•  ì :**\")\n","                        warnings = product_data.get('ì£¼ì˜í•  ì ', [])\n","                        if isinstance(warnings, list):\n","                            for warning in warnings:\n","                                st.markdown(f\"- {warning}\")\n","                        else:\n","                            st.markdown(f\"- {warnings}\")\n","                        st.divider()\n","            else:\n","                st.warning(\"ì„ íƒí•˜ì‹  ì¡°ê±´ì— ë§ëŠ” ì¶”ì²œ ì œí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\") # ì¶”ì²œ ì œí’ˆì´ ì—†ì„ ê²½ìš° ë©”ì‹œì§€\n","                st.markdown(\"ğŸ’¡ ì¶”ì²œ ê·¼ê±°: ìˆ˜ì§‘ëœ 15,000ê°œ ë¦¬ë·°ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ê´€ì‹¬ì‚¬ì™€ ë¼ì´í”„ìŠ¤íƒ€ì¼ì— ê°€ì¥ ì í•©í•œ ì œí’ˆì„ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤.\", unsafe_allow_html=True)\n","            st.markdown(\"</div>\", unsafe_allow_html=True)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"2cztwthFDoCU"},"execution_count":null,"outputs":[]}]}